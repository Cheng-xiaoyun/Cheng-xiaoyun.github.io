<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="丑小鸭的栖息地">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="丑小鸭的栖息地">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="丑小鸭的栖息地">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/">





  <title>丑小鸭的栖息地</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">丑小鸭的栖息地</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-15/" itemprop="url">2019-7-15</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:47:30+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Vocab.py功能：<br>就是对训练集进行word to index和index to word操作。对预训练的文件读取到一个矩阵表里面，同时进行word to index和index to word操作。<br>def get_embedding_weights(self,path):<br>这个方法作用：就是把预训练的语料那个txt文件进行迁移，迁移到一张表，这张表进行了word to index和index to word的索引操作。之后就可以通过这张表的id来引用需要的向量表示。</p>
<p>Word.txt预训练的词向量的文件从哪里来的（预训练好的词向量，word2vec，fasttext（发明者tomos mikolov。Fasttext改进的是用n-gram），glove）<br>为什么要根据外面预训练好的词向量文件建立一个词索引表，不直接在训练语料上建表？（预训练的涵盖的词范围更广，这样测试的时候oov就会少。如果直接用训练语料建表，测试的时候oov会很多。）<br>词向量空间距离反映语义相似度。</p>
<p>我们整体做的是什么：情感分析，文本分类。<br>模型思路：把模型、参数、语料传给分类器，分类器创建一个对象，然后对象有一个train方法，evaluate评估方法。</p>
<p>对待OOV：（1）置0；（2）随机值；（3）求已知向量的平均值</p>
<p>卷积，激活，池化封装为一序列的操作，进行窗口分别为3,4,5大小的三次卷积，然后将结果进行拼接。</p>
<p>把一层层的维度搞清楚，网络就是搞明白了：</p>
<p>卷积是从左到右，从上到下。<br>行是特征，列是索引。要转置一下再卷积。<br>卷积核池化进行维度变化，激活函数不会。<br>维度变化：<br>输入：<br>64<em>100<br>Embedding：<br>64</em>100<em>300<br>转置64</em>300<em>100，<br>然后卷积<br>64</em>75<em>100  64</em>100<em>100   64</em>125<em>100<br>池化：<br>64</em>75<em>1   64</em>100<em>1   64</em>125<em>1<br>拼接：<br>64</em>300<em>1<br>降维输入linear：<br>64</em>300</p>
<p>Batch_size<em>seq_len<br>Batch_size</em>seq_len<em>embedding<br>Batch_size</em>embedding<em>seq_len<br>Batch_size</em>embedding*1(pooling)</p>
<p>①python特点，一切兼为对象<br>②网络只能跑tensor，不能跑其他。 pytorch网络所有参数都是tensor，不存在numpy等其他类型。跑tensor也不单单是跑tensor，实际上跑的是variable变量，variable封装了data、grad梯度、创建方式<br>③在pytorch0.4之后，Tensor和variable合并了。<br>④List要用modulelist，不然不会报错，但是效果会差。<br>⑤模型参数 dilation是指用于控制内核点之间的距离，一般图像卷积会有用，文本卷积不需要。</p>
<p>写模型要看官方文档：<br>Pytorch<br><a href="https://pytorch-cn.readthedocs.io/zh/latest/" target="_blank" rel="noopener">https://pytorch-cn.readthedocs.io/zh/latest/</a><br><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/index.html</a><br>Python<br><a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener">https://www.runoob.com/python3/python3-tutorial.html</a></p>
<p>问题：<br>self._extwd2idx = {wd: idx+1 for idx,wd in enumerate(vec_tabs.keys())}</p>
<p>过拟合就是在训练集上很好，在测试集上很差。本质上就是因为①模型参数太多了②以复杂的模型去拟合少量的数据。<br>抑制过拟合：让数据又多又干净<br>①数据预处理（降噪，）<br>②dropout层（用在训练的时候）<br>③增大数据规模<br>加dropout原因：可能某些节点之间有依赖关系，是两个节点共同作用使结果变好。所以要用dropout随机的断掉一些节点，消除因为某些节点之间的依赖使得模型性能好。 让模型的泛华能力更强。<br>一般模型都加dropout。只有在训练的时候加。<br>④调参技巧：Eearlystopping早停<br>⑤weight_decay权值下降<br>⑥损失函数正则化</p>
<p>调参小结：<br>1.loss看趋势<br>2.训练集acc上升，loss下降表明训练正常<br>3.开发集acc下降，loss上升表明模型过拟合<br>4.Epoch不宜设的过大或过小，过大易过拟合，过小易欠拟合<br>5.Adam学习速率不宜过大（1e-3数量级）<br>6.Batch_size和lr尽量呈正相关<br>7.Dropout设定一般在【0.2,0.5】区间范围内<br>8.调参通过命令行去调整，把需要知道的信息打印出来<br>9.可以尝试使用grid seaarch穷举法调参方式<br>10.K-fold交叉验证</p>
<p>【my question 调优，反向传播，梯度更新，lr，weight decay，随机种子，loss】</p>
<p>Python train.py -h<br>Python Train.py –lr 1e-3 –epoch 32</p>
<p>–batch_size 64 –lr 0.0005  –epoc 20<br>文件里面设置的是默认值，通过命令行调参，看得到修改了哪些值，然后再去修改文件。</p>
<p>Pytorch为什么要梯度初始化置0：<br>梯度下降（权重w怎么更新的，梯度是一个向量，梯度方向是函数上升最快的方向。沿着梯度下降的方向找到局部最小值，对于凸优化问题，局部最小值就是全局最小值。）<br>Pytorch每求一次误差，梯度累加一次。<br>有了u误差更新梯度，有了梯度更新参数</p>
<p>Python关键字yield生成器：每次调用函数会接着上一次执行的结果执行后返回。和return不同，接着取上一次的数据同时还会把后面的函数执行完毕。取生成器里面的数据要通过for来取。</p>
<p>训练数据可能会存在前1000条是一个类别，后1000是一个类别，然后通过batch取得数据的时候取得的都是一个类型。所以在batch取数据的时候要进行shuffle。训练数据越乱越好，尽量包含多个类别数据。</p>
<p>把每个模块的目的搞清楚，然后自己写。</p>
<p>损失函数，用的较多的是MSE，交叉熵<br>weight_decay权重衰减<br>如果预测值w很大，<br>损失函数正则化：求损失的时候再加一个约束项（关于权重的）</p>
<p>My question<br>【一】batch数据变量化，什么二维切片<br>for i,inst in enumerate(batch_data):<br>    seq_len = len(inst.words)<br>    # 把实际长度覆盖掉<br>    wd_idx[i,:seq_len] = torch.tensor(wd_vocab.word2index(inst.words))<br>    lbl_idx[i] = torch.tensor( wd_vocab.label2index(inst.label))</p>
<p>【二】cpu  gpu<br>模型数据要放在同一个设备上。Train和datalodar里面都要加。Args。Device<br>【三】batch_X,batch_y = batch_variable(batch_data,self._vocab,self._args.device)<br>【四】# 用梯度去更新模型参数</p>
<h1 id="①加动量改变梯度方向；②改变学习速率（先快后慢）；"><a href="#①加动量改变梯度方向；②改变学习速率（先快后慢）；" class="headerlink" title="①加动量改变梯度方向；②改变学习速率（先快后慢）；"></a>①加动量改变梯度方向；②改变学习速率（先快后慢）；</h1><p>【五】weight_decay权重衰减<br>【六】# 计算准确率<br>def _calc_acc(self,pred,target):<br>    # pred dim = batch_size,label_size<br>    torch.eq(torch.argmax(pred,dim=1),target)<br>    pass</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-12/" itemprop="url">2019-7-12</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:46:49+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>【一】黄洪：字符串转换为id，word embedding。<br>预训练的词向量用于embedding。<br>映射：word对应一个id，一个id对应一个词向量。<br>首先预训练出一个词对应向量的词表。<br>然后embedding，pytorch里面。<br>写代码写代码</p>
<p>读取文件，变成string，vocab把word变成数字编号（连续有上界）。<br>Embedding矩阵可以随机初始化；也可以通过外面的词向量预训练得到。<br>【二】杨林：预训练得到词向量（语料库）Embedding怎么来的。<br>fastText提供了集中训练词向量的模型。Fasttext包含了Word2vec。<br>Skipgram模型：找到一个词的上下文信息，也就是语义信息。<br>官方文档参数解释：mincount（规定一个词出现的最小次数，只有达到这个数值的词才可以归到语料库里面）wordNgrams（就是把词比如2gram就是随机把一句话的两个词进行组合。3gram就是随机3个词进行组合。）bucket（和哈希有关系，加快训练查找什么的）minn maxn（规避oov问题）dim（训练的向量维度）lr（learning right在拟合的过程中，首先要在损失函数上面选定一个点，目标是逼近最低点，lr就是长度，每次逼近时移动的长度）loss（损失函数）ws（context window就是怎么定义上下文）epoch（训练多少轮）neg（一般选5）pretrainvector是目标。</p>
<p>【三】吴林志<br>神经网络，首先将输入节点做一个线性或非线性变化，然后激活函数，输出。Y=g(wx+b)。<br>函数就是映射，假设有一个函数f(“cat”)=猫。<br>深度学习最基本的运算就是矩阵。<br>常见激活函数tanh，symoid，relu（公式下去了解）。<br>n多个节点的话：<br>如果每个节点和下一个节点都有连接，就是输入层、隐层、输出层。<br>不全连接，只是部分连接，参数很多，这就是卷积网络（局部连接，权值共享）<br>卷积网络：卷积核（就是输入序列取多少个节点，窗口大小，）<br>卷积操作：加权求和，随着窗口移动对应位置相乘，然后进行卷积结果拼接。</p>
<p>通过窗口局部卷积，卷积一般都是向上取整，填充为了保证输入输出一致。卷积核固定情况下，通过pad做伸缩，二维里面就是保证输出形状基本不变，同时信息提取的更充分。<br>池化层：跟卷积操作一样，不一样的就是对卷积的结果进行处理，一般都是取最大值或者取平均值。<br>经典的卷积：先做一个卷积层，然后做一个线性变化relu（假如卷积信息有效就保留，无用就筛选），最后经过一个池化层（池化层看情况加，大多数情感分析都加池化层。对图像来说，池化目的就是从一堆特征选出一个更好的特征。对于文本来说池化就是得出一个固定的值）。</p>
<p>加激活函数是为了让模型非线性化。不加的话加再多层也只是个线性组合。</p>
<p>总的一个设计思路：<br>1.面向对象、模块化<br>2.设计过程，增量开发<br>3.从整体到局部</p>
<p>Python pytest单元测试：<br>1.py文件以testXXX命名<br>2.方法名也是testXXX</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-11/" itemprop="url">2019-7-11</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:45:54+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>重视实践<br>比如文本分类重要的是情感分析，不能只从书上知道，要实践。</p>
<p><a href="https://nlp.stanford.edu/projects/" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/</a><br>培训两个月之后要交一个技术报告，要写的很详细具体，比如你对attention机制的理解，你觉得深度学习最重要的部分。<br><a href="https://arxiv.org/list/cs.CL/recent" target="_blank" rel="noopener">https://arxiv.org/list/cs.CL/recent</a></p>
<p>要用pytorch<br>大的方向：文本分类任务</p>
<p>下午：<br>【一】黄洪部分：<br>从数据集拿到数据，到扔到模型之前的处理。<br>主要步骤：<br>（1）数据读取加载。<br>可以做分词的nltk,spacy。<br>数据加载：Dataloader类；统计词的频率：Counter类。<br>根据任务进行分类，比如二分类就标记0,1。标签和文本之间要通过符号分割。<br>（2）预处理。<br>①进行符号的统一，比如中英文的标点符号不一样会影响歧义。空格相关的也进行处理。就是把后面的文本规范化。<br>②文本加载，pytorch有一个自带的类可以自动加载（但是是黑盒的，要保证完全理解它的源码，完全掌握可以用，甚至可以自己拓展）。手动写的话比较灵活，可以按照你定义的规则来组成一个instance，就像我们检测代码克隆读取一个一个方法，通过大括号来提取。<br>（3）One-hot编号。Word to Id，词转换成id。<br>Word embedding<br>（4）生成batch（批处理，在机器学习上不是必须的。但是深度学习数据量比较大，）。通过调用pytorch。</p>
<p>【二】张老师：<br>训练、开发、测试，防止以后做开发出现误区：<br>不要怕出错，在错误中学习。<br>在训练集上调参，不是在测试集上调参。<br>神经网络有很多不好的地方，随机种子换一下结果就会很不一样。数据标的话一致性比较差，训练样本比较差。所以要换几个随机种子，最后取结果的平均值。<br>或者是交叉验证大参数不需要调，小参数比如迭代次数多次调换。<br>预训练可以规避随机种子的问题，但是预训练和真正训练的数据集是不一样的。<br>传统机器学习方法（在深度学习之前）：<br>Vocab结构就是把字符串转换为数字，字符串可以是word、lable。<br>“我 今天 早上 起 得 很  早”<br>在深度学习之前：每个词一个编号，对应一个10000维的向量（one-hot向量，非常稀疏，只有一个部分是1，其它部分是0）。<br>然后情感分析o=w*x（把一句话转换为一个10000维的向量，哪一位有词就是1没有就是0 ）<br>主要是训练w参数，模型就是参数。<br>深度学习：Word embedding</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/29/transformer/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/29/transformer/transformer/" itemprop="url">Transformer初步了解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-29T16:51:18+08:00">
                2019-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transformer/" itemprop="url" rel="index">
                    <span itemprop="name">transformer</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且<strong>仅由self-Attenion和Feed Forward Neural Network组成</strong>。<br>采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：<br>       1.时间片 t 的计算依赖 t-1 时刻的计算结果，这样限制了模型的并行能力；<br>       2.顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。<br>Transformer的提出解决了上面两个问题，<strong>首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量</strong>；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。<br><img src="https://img-blog.csdnimg.cn/20190929161612774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如论文中所设置的，编码器由6个编码block组成，同样解码器是6个解码block组成。与所有的生成模型相同的是，编码器的输出会作为解码器的输入，如图3所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190929161631536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929161647206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Decoder的结构如图5所示，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：<br>       1. <strong>Self-Attention：当前翻译和已经翻译的前文之间的关系；</strong><br>        2. Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。<br><img src="https://img-blog.csdnimg.cn/20190929161703777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="输入编码"><a href="#输入编码" class="headerlink" title="输入编码:"></a>输入编码:</h2><p><strong>Word2Vec</strong>等词嵌入方法将输入语料转化成特征向量，论文中使用的词嵌入的维度为d=512;<br><img src="https://img-blog.csdnimg.cn/20190929161713284.png" alt="在这里插入图片描述"><br>在最底层的block中， x 将直接作为Transformer的输入，而在其他层中，输入则是上一个block的输出。<br><img src="https://img-blog.csdnimg.cn/20190929161726274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-Attention是Transformer最核心的内容，<br>在self-attention中，<strong>每个单词有3个不同的向量</strong>，它们分别是Query向量(Q),Key向量(k)和Value向量(v),，长度均是64。它们是通过3个不同的权值矩阵由嵌入向量X(1<em>512)乘以三个不同的权值矩阵WQ,WK,WV(512</em>64)得到的。<br><img src="https://img-blog.csdnimg.cn/20190929161736488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>那么Query，Key，Value是什么意思呢？它们在Attention的计算中扮演着什么角色呢？我们先看一下Attention的计算方法，整个过程可以分成7步：<br><img src="https://img-blog.csdnimg.cn/2019092916154161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上面步骤的可以表示为图10的形式。<br><img src="https://img-blog.csdnimg.cn/20190929161954345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>实际计算过程中是采用基于矩阵的计算方式，那么论文中的 Q,K,V的计算方式如图11：<br><img src="https://img-blog.csdnimg.cn/20190929162256228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929162320528.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在self-attention需要强调的最后一点是其采用了<strong>残差网络 中的short-cut结构</strong>，目的当然是解决深度学习中的退化问题，得到的最终结果如图13。<br><img src="https://img-blog.csdnimg.cn/20190929162440763.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929163721155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929163806782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>同self-attention一样，multi-head attention也加入了short-cut机制。</p>
<h2 id="Encoder-Decoder-Attention"><a href="#Encoder-Decoder-Attention" class="headerlink" title="Encoder-Decoder Attention"></a>Encoder-Decoder Attention</h2><p>在解码器中，Transformer block比编码器中多了个encoder-cecoder attention。在encoder-decoder attention中， Q 来自与解码器的上一个输出， K 和 V则来自于与编码器的输出。其计算方式完全和图10的过程相同。</p>
<p>由于在机器翻译中，解码过程是一个顺序操作的过程，也就是当解码第 k 个特征向量时，我们只能看到第 k-1 及其之前的解码结果，论文中把这种情况下的multi-head attention叫做masked multi-head attention。</p>
<h2 id="损失层"><a href="#损失层" class="headerlink" title="损失层"></a>损失层</h2><p>解码器解码之后，解码的特征向量经过一层激活函数为softmax的全连接层之后得到反映每个单词概率的输出向量。此时我们便可以通过CTC等损失函数训练模型了。</p>
<p>而一个完整可训练的网络结构便是encoder和decoder的堆叠（各 N个， N=6），我们可以得到图15中的完整的Transformer的结构（即论文中的图1）<br><img src="https://img-blog.csdnimg.cn/20190929164330695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>截止目前为止，我们介绍的Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。</p>
<p>为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。</p>
<p>那么怎么编码这个位置信息呢？常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。那么这个位置编码该是什么样子呢？通常位置编码是一个长度为 d(512维) 的特征向量，这样<strong>便于和词向量进行单位加</strong>的操作，如图16。<br><img src="https://img-blog.csdnimg.cn/2019092916470993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929164836701.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>优点</strong>：（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。（2）Transformer的设计最大的带来性能提升的关键是将<strong>任意</strong>两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</p>
<p><strong>缺点</strong>：（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉<strong>局部特征</strong>的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。（2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/29/网易云学习/Attention机制/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/29/网易云学习/Attention机制/" itemprop="url">Attention机制</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-29T11:09:41+08:00">
                2019-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nn-basic/" itemprop="url" rel="index">
                    <span itemprop="name">nn basic</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Attention机制其实就是一系列注意力分配系数，也就是一系列权重参数罢了。</p>
<p>注意力模型就是要从序列中学习到每一个元素的重要程度，然后按重要程度将元素合并。 </p>
<p>换一个角度来理解，我们将attention机制看做软寻址。就是说序列中每一个元素都由key(地址)和value(元素)数据对存储在存储器里，当有query=key的查询时，需要取出元素的value值(也即query查询的attention值)，与传统的寻址不一样，它不是按照地址取出值的，它是通过计算key与query的相似度来完成寻址。这就是所谓的软寻址，它可能会把所有地址(key)的值(value)取出来，上步计算出的相似度决定了取出来值的重要程度，然后按重要程度合并value值得到attention值，此处的合并指的是加权求和。</p>
<p>优点:<br>一步到位的全局联系捕捉</p>
<p>上文说了一些，attention机制可以灵活的捕捉全局和局部的联系，而且是一步到位的。另一方面从attention函数就可以看出来，它先是进行序列的每一个元素与其他元素的对比，在这个过程中每一个元素间的距离都是一，因此它比时间序列RNNs的一步步递推得到长期依赖关系好的多，越长的序列RNNs捕捉长期依赖关系就越弱。<br>并行计算减少模型训练时间</p>
<p>Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。但是CNN也只是每次捕捉局部信息，通过层叠来获取全局的联系增强视野。</p>
<p>缺点:<br>缺点很明显，attention机制不是一个”distance-aware”的，它不能捕捉语序顺序(这里是语序哦，就是元素的顺序)。这在NLP中是比较糟糕的，自然语言的语序是包含太多的信息。如果确实了这方面的信息，结果往往会是打折扣的。说到底，attention机制就是一个精致的”词袋”模型。所以有时候我就在想，在NLP任务中，我把分词向量化后计算一波TF-IDF是不是会和这种attention机制取得一样的效果呢? 当然这个缺点也好搞定，我在添加位置信息就好了。所以就有了 position-embedding(位置向量)的概念了，这里就不细说了。</p>
<p>Attention机制说大了就一句话，分配权重系数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/29/网易云学习/RNN的梯度下降and使用GRU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/29/网易云学习/RNN的梯度下降and使用GRU/" itemprop="url">RNN的梯度下降&使用GRU</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-29T11:07:57+08:00">
                2019-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nn-basic/" itemprop="url" rel="index">
                    <span itemprop="name">nn basic</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>长期的依赖关系：前面的单词，对句子后面的单词有影响。<br>RNN不擅长捕获这种长期的关系，<br>层数很深的时候，反向传播，后面层的输出误差，很难影响前面层的权重。<br>梯度爆炸很容易发现，你的参数会随着指数增加。（解决办法：梯度修剪，观察你的梯度向量，如果他高于某个数值，就缩放梯度向量）<br>但是梯度消失很难解决。</p>
<p>GRU：门控循环控制单元<br>能够解决梯度消失问题，捕获长期依赖关系。</p>
<p>RNN隐藏层的可视化：<br><img src="https://img-blog.csdnimg.cn/20190929110559646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>上一个时间步的激活值和当前的x输入一起输入一个激活函数tanh，b是当前的偏差值，计算出激活值a，然后输入一个softmax之类的函数，得到y。</p>
<p>GRU单元：<br>有个新的变量c记忆细胞，<br>对于GRU<br><img src="https://img-blog.csdnimg.cn/20190929110615150.png" alt="在这里插入图片描述"><br>计算激活值的函数：这是一个候选值，即c<t>的值<br><img src="https://img-blog.csdnimg.cn/20190929110623738.png" alt="在这里插入图片描述"><br>GRU有一个更新门，数值在0-1之间，就是把计算激活值得函数带入sigmoid函数。<br><img src="https://img-blog.csdnimg.cn/20190929110634338.png" alt="在这里插入图片描述"><br>门来决定是否要真的更新。<br>门的值 乘以 候选值 加 （1 减去门的值）乘旧的值<br><img src="https://img-blog.csdnimg.cn/2019092911064375.png" alt="在这里插入图片描述"><br>如果不需要更新就把门的值记为0，就是继续使用旧的值。需要更新就置为1，使用候选值。<br>比如下图，根据cat的单复数影响后面的was还是were。当遇到cat的时候门的值为1。然后后面的单元 which already什么的都是0。<br>c不更新，但是他一直记得cat是单数还是复数。</t></p>
<p><img src="https://img-blog.csdnimg.cn/20190929110652701.png" alt="在这里插入图片描述"><br>GRU的门控单元<br><img src="https://img-blog.csdnimg.cn/20190929110700873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>其中紫色阴影部分就是这个公式：<br><img src="https://img-blog.csdnimg.cn/20190929110711422.png" alt="在这里插入图片描述"></p>
<p>GRU的优点就是，通过门决定，当你从左到右扫描这个句子的时候，是否要更新记忆单元，不更新直到你真的需要使用记忆细胞的时候。<br>因为门值很接近0，c<t>很接近c<t-1>，c的值很好地被维持了，即使经过很多层。这就是缓解梯度下降。</t-1></t></p>
<p>GRU单元三个重要的公式：</p>
<p><img src="https://img-blog.csdnimg.cn/20190929110739291.png" alt="在这里插入图片描述">是候选值，中间加一个 门控值 来告诉我们计算出下一个c值和上一个的相关性有多大（比如was were和cat、cats关联大）</p>
<p><img src="https://img-blog.csdnimg.cn/20190929110752651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/29/网易云学习/神经网络基本结构and语言模型的构建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/29/网易云学习/神经网络基本结构and语言模型的构建/" itemprop="url">神经网络基本结构&语言模型的构建</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-29T11:04:18+08:00">
                2019-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nn-basic/" itemprop="url" rel="index">
                    <span itemprop="name">nn basic</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="https://img-blog.csdnimg.cn/20190929110335169.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>one to one：不重要，没什么可说的<br>one to many：比如音乐生成（序列模型）<br>many to one：比如情感分析，输入一段评价，生成一个星级的负面或正面。<br>many to many（输入和输出长度一致）<br>many to many（输入和输出长度不一致）：比如机器翻译，一段文字翻译为另外一种语言，encoder-decoder。<br>注意力机制：<br>把他们组合在一起，就可以生成各种各样的模型。</p>
<p>语言模型：输入一个文本序列，语言模型用y表示x，并且估计每个句子中，各个单词出现的可能性。<br>如何构建语言模型：<br>第一个时间步：<br>输入o向量。然后利用softmax预测第一个词的概率。而不去管结果。softmax可能会输出10000（你的词典大小）种可能。<br>第二个的时间步：<br>传入正确的第一个单词和上一步预测的单词，计算出序列的下一个单词是什么，通过之前的输入，计算出字典中的每一个词的概率。<br>RNN：考虑前面得到的所有词，给出下一个词的分布，从左到右的预测每个词。<br><img src="https://img-blog.csdnimg.cn/20190929110404727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>训练一个好的模型：<br>定义一个代价函数，也就是损失函数。整体损失就是每一步损失相加起来。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/25/私密/For Mr.Hou2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/25/私密/For Mr.Hou2/" itemprop="url">Mr.Hou's mistake</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-25T16:35:28+08:00">
                2019-09-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/hou/" itemprop="url" rel="index">
                    <span itemprop="name">hou</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>太多了，数不清，自己反思去哇</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/15/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/15/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-15T16:45:47+08:00">
                2019-09-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/test/" itemprop="url" rel="index">
                    <span itemprop="name">test</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/15/私密/For Mr.Hou/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/15/私密/For Mr.Hou/" itemprop="url">don't look</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-15T16:45:28+08:00">
                2019-09-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/hou/" itemprop="url" rel="index">
                    <span itemprop="name">hou</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>welcome 侯先生：</strong></p>
<ul>
<li><p>你是小可爱的第一个访客哟。</p>
<p>爱你(＾Ｕ＾)ノ~ＹＯ<img src="https://img-blog.csdnimg.cn/20190913183216323.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/head.gif" alt="Chengxiaoyun">
            
              <p class="site-author-name" itemprop="name">Chengxiaoyun</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chengxiaoyun</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
