<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="丑小鸭的栖息地">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="丑小鸭的栖息地">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="丑小鸭的栖息地">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>丑小鸭的栖息地</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">丑小鸭的栖息地</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-24/" itemprop="url">2019-7-24</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:49:47+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>经典序列标注任务<br>自然语言任务：分类任务、结果学习任务。<br>词性标注：我（名词） 家 养（动词） 了 只（量词） 猫。标签之间也是有依赖关系的，比如名词后面经常跟动词。</p>
<p>【分词】<br>中文、日语词语词之间是没有间隔的。把分词转换为序列标注问题<br>例子一：“天津大学新校区位于津南区” 进行分词<br>几种分词标签：①Single就是单个标签，比如“猫”②Begin比如“天津大学”③Middle“津大”④End。<br>“小 明 家 养 了 一 只 猫”<br>“B  E  S  B  E  B  E  S”<br>标签之间有一些依赖和限制，比如S不可能接M或者E。<br>通常是在LSTM接一个CRF用来解决标签之间相互转换为关系。比如要对“天津大学新校区位于津南区”进行分词，CRF模型对词进行打分，比如“南”这个字被预测为B E M S的概率。对于“津”的标签预测是和“南”字有关联的，所以在对“津”打分的时候，分为两部分，第一部分是这个字被预测为B E M S的概率，另一部分是前一个词“南”被。<br>求解空间是4的n次方，n是这句话有几个字，每个词被预测为4种标签的概率。假如每个字之间没有任何依赖关系，想得到最好的预测，最优子结果不仅包括每个词的预测，还有每个词之间的联系也就是边的值。<br>怎么做训练：</p>
<p>【补充】<br>Shape属性。查看矩阵或者数组的维数，shape[0] 为第一维的长度，shape[1] 为第二维的长度<br>Reshape()函数重新定义了原张量的阶数：给数组一个新的形状而不改变其数据。通过reshape生成的新数组和原始数组公用一个内存，也就是说，假如更改一个数组的元素，另一个数组也将发生改变 。</p>
<p>a=np.array([1,2,3,4,5,6,7,8,9,10,11,12])<br>b=np.reshape(a,(2,-1))<br>c=np.reshape(a,(2,2,-1))<br>d=np.reshape(a,(2,3,-1))<br>b=<br>[[ 1  2  3  4  5  6]<br> [ 7  8  9  10 11 12]]<br>c=<br>[[[ 1  2  3]<br>  [ 4  5  6]]</p>
<p> [[ 7  8  9]<br>  [10 11 12]]]<br>d=<br>[[[ 1  2]<br>  [ 3  4]<br>  [ 5  6]]</p>
<p> [[ 7  8]<br>  [ 9 10]<br>  [11 12]]]</p>
<p>Squeeze<br>Unsqueeze<br>Size（）</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-23/" itemprop="url">2019-7-23</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:49:17+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>常见文本关系：<br>文本蕴涵（有方向性，侧重在语义，A推出B，B退不出A）、文本相似（文本字面结构上的相似，不是说语义）、文本复述（文本蕴涵的一种特殊情况，A能推出B ，B能推出A）、逻辑推理（侧重在逻辑方面，文本蕴涵侧重在语义方面）</p>
<p>自然语言的一些任务，任务之一文本蕴涵：<br>【一】文本蕴涵（判断两个句子）三种叫法<br>STS semantic sentence similarity 判断两个句子语义相似性<br>RTE 判断两个句子，一个包含另外一个<br>NLI 两个句子，一个前提一个假设，能否通过一个推出另外一个<br>【二】文本蕴涵的三种标签<br>Entailment蕴含关系：意思相同，A推出B，<br>Neural 中性关系：没有任何关系<br>Contradiction反对关系：意义相反，矛盾<br>【三】文本蕴涵应用场景<br>①问答<br>②信息检索<br>③关系识别<br>④知识获取<br>⑤蕴含对生成<br>【四】文本蕴涵任务里面公开的数据集（都是英文的）<br>MSRP 从一些新闻文章中提取的数据集，里面有很多复杂的语义。<br>Quora 本质上是一个知识分享网站，所有的句子都是一个问题，然后判断这两个问题的关系。<br>SICK   在一个网站上发表一些任务，然后各界人来做标注。<br>SNLI   人工书写的英语子对集合。数据集很大。<br>Scitall 有一个问题，多个选项，选择一个正确答案，然后转换为断言陈述。<br>【五】方法<br>TF-IDF 判断两句话的相关性<br>ESIM模型：有两种（双向LSTM、基于树的LSTM）<br>【六】特征<br>词频特征、词嵌入特征</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-22/" itemprop="url">2019-7-22</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:48:57+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>张老师：<br>【一】<br>Embedding就是一个表，包含词和索引。<br>每次查表，找对应词的向量表示，就是一个operation。<br>不管函数多复杂，总是可以通过分解为一些原子操作，比如查表、线性变化、<br>三种基本运算：<br>①Embedding<br>②Wx<br>③w——x——h——RNN（pooling）——hp——wx（分类、线性变化）<br>整个预测，本质上就是一个函数MLP—pooling—BMLP—MLP—embedding。把这个函数打开，就像是一个图，按照图的拓扑顺序。<br>深度学习就是一个复合函数，为每个原子（节点）定义一个input——forward——output——backward——input。<br>整个神经网络就是模仿一个复合函数。<br>机器学习怎么来：<br>目标函数怎么定义：交叉熵。熵越大损失越大。<br>优化损失值：梯度下降。要把损失往回传，<br>定义好MLP的偏导，就是backward，然后沿着拓扑反方向一层一层往前求。复合函数求导就是深度学习的根基。<br>如果自己不写网络都不需要管backward。<br>Forward：Y=wx+b<br>Backward：求导</p>
<p>LSTM、RNN不能并行计算因为每一步都与前面有关联，但是Transform可以并行计算。<br>Op的概念：每个op都有forward、backward。<br>拓扑的概念：每个句子是一棵树，但是多个句子就是图，并行一定要高。<br>优化的概念：不搞机器学习都不用管。</p>
<p>哪些工作取决于编码能力。<br>【二】<br>性能好：①模型本身；②输入有多丰富。<br>词向量获取Pre trainning<br>语言模型任务：通过“我 喜欢 * 苹果”预测“吃”这个词。这样的语料无穷无尽，任何一个合理的句子都会提供一个语料。选取高频的词。最简单的词向量模型CBow，就是一个分类问题，难点就是预测。预训练的语调要够大。<br>Skipgram正好相反，用“吃”这个词预测它的上下文。</p>
<p>Deep bilstm：情感分析，词性标注，实体识别，句法分析，文本蕴涵，……<br>语言模型：几乎无限的预训练语料。</p>
<p>Language Model<br>ELMO目前最好用的词向量。<br>BERT（transform代替lstm）<br>改进：①Lstm有方向，transform没有方向。②③超大规模数据测。④精细的调参技术。<br>吴林志<br>【三】Char Embedding<br>把字符特征加到 表示信息更丰富。拼接字符级特征。<br>字符词向量没有预训练好的表，都是随机初始化的。<br>训练语调统计的字符不包含全部，所以把字符表建立的完备一点。<br>如果embedding_dim是300维度 char_embed_dim是200维度，那最后拼接就是500 维度，包含了字符特征。</p>
<p>标准正太分布，方差<br> 初始化可以用xavier_normal，会根据前一层节点，当前一层有n个节点的时候，根据标准差为根号1/n进行正态分布初始化，当激活函数tanh、s型曲线的时候用这个<br> 初始化也可以用kaiming_normal，对xavier_normal的改进，标准差乘2进行正态分布，当激活函数relu的时候用这个<br>一般用kaiming_normal，因为大多数激活函数是relu</p>
<p>shape是一个属性，size是方法。</p>
<p>Python  train – char_hidden_size 150 –char_embed_dim 50<br>【问题1】<br>每个索引如果是3维度，经过embedding就变成4维<br>【问题2】<br>conv_out =torch.cat( [self._convs(embed) for _convs in self._convs],dim=1).squeeze(dim=2)<br>【问题3】<br>out.reshape(batch_size, max_seq_len ,-1)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-19/" itemprop="url">2019-7-19</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:48:12+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Transformer<br>Seq——seq机器翻译模型<br>中文——encoder编码——decorder解码——英文<br>Attention只是局部提取信息，每个信息之间没有连接起来。循环结构不需要加位置信息，因为序列循环已经包含了位置信息。但是卷积就需要位置信息，序列的先后顺序，因为他学习不到位置信息。可以给每个节点加一个位置信息，positional encoding。</p>
<p>Attention注意力机制：<br>不是一种模型，而是一种思想。有翻译就一定要用到attention。<br>“我喜欢恐怖片”是正面的，因为人的大脑关注的是局部信息，人们注意到的是“喜欢”这个词。深度学习也是从大量信息中选择需要的重要信息。之前的cnn rnn只学习到“喜欢”这个局部特征，没有考虑情感词。怎么体现这个选择性呢，通过权重，当遇到“喜欢”这个词的时候就权重分配大一点。<br>给序列分配权重：<br>通过点乘，或者余弦，去找到下一步的权重。找到权重之后加权求和。</p>
<p>这是一个模型，没有具体含义，在具体问题中才有含义。<br>用一个外部的查询变量，查询序列Q K的相似度（相似度也就是和哪个关联最大），越相似的权重越高，然后施加到V上面。除以根号dk是防止过大。相似度同时除以一个值，不影响相似度。<br>【相似性：从英文（Q）——attention——翻译为法文(K)<br>从历史隐层信息，根据已经翻译的结果，去找序列里面哪个相似度最高的来找下一个，Q里面有学好的上下文历史信息，帮助预测下一步，权重怎么分配是根据已经学好的历史信息】<br>用一个外部的查询变量，先通过点乘找相似度，通过softmax概率化归一化变成概率，使得和为1.然后施加到V上面求和。</p>
<p>Lstm没有学到局部信息，只学习到了序列信息。<br>Self attention：<br>是Q没有用到外部信息，只用到了自身信息。Q=K=V。只寻找序列内部的联系。<br>（I like apple and it is dilicious自己学习，网络把dilicious指向apple。）无视词之间的距离，直接计算距离关系，能够学习到一个句子的内部结构。直接找，不需要从0时刻开始把前面的信息累计起来，比如dilicious直接找apple。卷积就是直接卷积，没有迭代，不需要把前面执行完，比lstm序列模型速度快。<br>点乘可以并行。矩阵乘不能做并行（因为矩阵是行元素乘列元素累加）。能否做并行就看他的局部变量是否是独立的。<br>残差连接：<br>是<br>Encoder编码：<br>是</p>
<p>Maxpooling本身就是一个attention，取最大元素，就是注意力。<br>要用attention需要外部辅助信息，很多时候没有外部信息，更多时候是用自己的信息去产生权重，也就是self attention，attention重点就是权重。方法就是利用末状态的隐藏层信息h——next，h和c都可以，h就是对c做了线性变化，h涵盖了c。</p>
<p>采用attention依旧存在的问题：<br>（1）序列问题存在一个对齐问题，有的序列是做了填充padding的，有可能最后几个元素是填充的，但是也被分了权重，使得原来有元素的位置权重被分了。<br>解决办法：mask<br>在softmax的时候分配权值，控制padding填充部分不分配权值。把padding部分传入负无穷。</p>
<p>使用服务器设置参数–cuda 1</p>
<p>My question：<br>【1】mask，传入负无穷，就可以分配权值为0。-mask就是padding部分，然后填充为负无穷。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-17/" itemprop="url">2019-7-17</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:47:48+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。<br>循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>
<p>捕捉序列信息，循环神经网络，常见的有LSTM，GRU（lstm的变体），RNN。<br>把之前的信息不断累积，通过一个迭代结构，把之前的序列信息，加上当前时刻的信息。历史信息都学到了。加了一个循环的结构。<br>与传统不同的是：<br>引入了一个新的状态累计信息，引入了一个门机制。<br>可能存在的问题：<br>训练时更新权重：梯度爆炸 梯度离散<br>小于0的数越乘越小，或者是越来越大。两种极端。<br>出现这种异常梯度的原因是序列太长的话，就只能学习到几步，梯度障碍。再往前追溯追溯不了。</p>
<p>几种门机制：输入门，输出门，遗忘门（控制前一时刻多少信息流入）</p>
<p>底层的网络能捕获基本信息，层数加深就能捕获到语义信息。双向网络（有的信息是后缀的，必须从两个方向去捕捉语义，比如“衣服 差 的 不行”。BiLSTM）</p>
<p>reverse()是一个list的反转。reversed是一个内置的反转。</p>
<p>Question：<br>【1】什么元祖什么的</p>
<h1 id="lstmcell-grucell-fw-next-不是元祖-h1-c1迭代"><a href="#lstmcell-grucell-fw-next-不是元祖-h1-c1迭代" class="headerlink" title="lstmcell/grucell   fw_next 不是元祖 h1=c1迭代"></a>lstmcell/grucell   fw_next 不是元祖 h1=c1迭代</h1><p>for xi in range(seq_len):<br>    if self._rnn_type == ‘LSTM’:<br>        # lstmcell   fw_next也是元祖h1 c1  h0 c0<br>        h_next,c_next = self._rnn_cell(inputs[xi],fw_next)<br>        # 先拆开后合并，做一个过滤<br>        h_next = h_next * mask[xi]<br>        c_next = c_next * mask[xi]<br>        fw_next = (h_next,c_next)<br>        outputs.append(h_next)<br>    else:<br>      # RNNcell/GRUcell   fw_next 不是元祖 h1=c1迭代<br>      fw_next = self._rnn_cell(inputs[xi], fw_next)<br>      fw_next = fw_next * mask[xi]<br>      outputs.append(fw_next)</p>
<p>【2】掩码填充什么的<br>【3】def _forward(self,inputs,init_hidden,mask):<br>    ‘’’<br>    :param inputs: inputs[seq_len,batch_size,input_size]<br>    :param init_hidden:[batch_size,hidden_size],如果是lstm，则init_hidden类型是元祖<br>    :param mask: inputs[seq_len,batch_size,hidden_size]<br>    :return:<br>    ‘’’<br>也可以这样处理mask  h_next = h_next * mask[xi]+init_hidden[0]<em>(1-mask[xi])<br>【4】#  outputs保留的是中间状态，每个大小都是batch_size</em>hidden_size<br>【5】为什么要返回三维的torch.stack(tuple(outputs),dim=0)<br>【6】bilstm和lstm  rnn的关系，为什么在bilstm里面调用rnn</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-15/" itemprop="url">2019-7-15</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:47:30+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Vocab.py功能：<br>就是对训练集进行word to index和index to word操作。对预训练的文件读取到一个矩阵表里面，同时进行word to index和index to word操作。<br>def get_embedding_weights(self,path):<br>这个方法作用：就是把预训练的语料那个txt文件进行迁移，迁移到一张表，这张表进行了word to index和index to word的索引操作。之后就可以通过这张表的id来引用需要的向量表示。</p>
<p>Word.txt预训练的词向量的文件从哪里来的（预训练好的词向量，word2vec，fasttext（发明者tomos mikolov。Fasttext改进的是用n-gram），glove）<br>为什么要根据外面预训练好的词向量文件建立一个词索引表，不直接在训练语料上建表？（预训练的涵盖的词范围更广，这样测试的时候oov就会少。如果直接用训练语料建表，测试的时候oov会很多。）<br>词向量空间距离反映语义相似度。</p>
<p>我们整体做的是什么：情感分析，文本分类。<br>模型思路：把模型、参数、语料传给分类器，分类器创建一个对象，然后对象有一个train方法，evaluate评估方法。</p>
<p>对待OOV：（1）置0；（2）随机值；（3）求已知向量的平均值</p>
<p>卷积，激活，池化封装为一序列的操作，进行窗口分别为3,4,5大小的三次卷积，然后将结果进行拼接。</p>
<p>把一层层的维度搞清楚，网络就是搞明白了：</p>
<p>卷积是从左到右，从上到下。<br>行是特征，列是索引。要转置一下再卷积。<br>卷积核池化进行维度变化，激活函数不会。<br>维度变化：<br>输入：<br>64<em>100<br>Embedding：<br>64</em>100<em>300<br>转置64</em>300<em>100，<br>然后卷积<br>64</em>75<em>100  64</em>100<em>100   64</em>125<em>100<br>池化：<br>64</em>75<em>1   64</em>100<em>1   64</em>125<em>1<br>拼接：<br>64</em>300<em>1<br>降维输入linear：<br>64</em>300</p>
<p>Batch_size<em>seq_len<br>Batch_size</em>seq_len<em>embedding<br>Batch_size</em>embedding<em>seq_len<br>Batch_size</em>embedding*1(pooling)</p>
<p>①python特点，一切兼为对象<br>②网络只能跑tensor，不能跑其他。 pytorch网络所有参数都是tensor，不存在numpy等其他类型。跑tensor也不单单是跑tensor，实际上跑的是variable变量，variable封装了data、grad梯度、创建方式<br>③在pytorch0.4之后，Tensor和variable合并了。<br>④List要用modulelist，不然不会报错，但是效果会差。<br>⑤模型参数 dilation是指用于控制内核点之间的距离，一般图像卷积会有用，文本卷积不需要。</p>
<p>写模型要看官方文档：<br>Pytorch<br><a href="https://pytorch-cn.readthedocs.io/zh/latest/" target="_blank" rel="noopener">https://pytorch-cn.readthedocs.io/zh/latest/</a><br><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/index.html</a><br>Python<br><a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener">https://www.runoob.com/python3/python3-tutorial.html</a></p>
<p>问题：<br>self._extwd2idx = {wd: idx+1 for idx,wd in enumerate(vec_tabs.keys())}</p>
<p>过拟合就是在训练集上很好，在测试集上很差。本质上就是因为①模型参数太多了②以复杂的模型去拟合少量的数据。<br>抑制过拟合：让数据又多又干净<br>①数据预处理（降噪，）<br>②dropout层（用在训练的时候）<br>③增大数据规模<br>加dropout原因：可能某些节点之间有依赖关系，是两个节点共同作用使结果变好。所以要用dropout随机的断掉一些节点，消除因为某些节点之间的依赖使得模型性能好。 让模型的泛华能力更强。<br>一般模型都加dropout。只有在训练的时候加。<br>④调参技巧：Eearlystopping早停<br>⑤weight_decay权值下降<br>⑥损失函数正则化</p>
<p>调参小结：<br>1.loss看趋势<br>2.训练集acc上升，loss下降表明训练正常<br>3.开发集acc下降，loss上升表明模型过拟合<br>4.Epoch不宜设的过大或过小，过大易过拟合，过小易欠拟合<br>5.Adam学习速率不宜过大（1e-3数量级）<br>6.Batch_size和lr尽量呈正相关<br>7.Dropout设定一般在【0.2,0.5】区间范围内<br>8.调参通过命令行去调整，把需要知道的信息打印出来<br>9.可以尝试使用grid seaarch穷举法调参方式<br>10.K-fold交叉验证</p>
<p>【my question 调优，反向传播，梯度更新，lr，weight decay，随机种子，loss】</p>
<p>Python train.py -h<br>Python Train.py –lr 1e-3 –epoch 32</p>
<p>–batch_size 64 –lr 0.0005  –epoc 20<br>文件里面设置的是默认值，通过命令行调参，看得到修改了哪些值，然后再去修改文件。</p>
<p>Pytorch为什么要梯度初始化置0：<br>梯度下降（权重w怎么更新的，梯度是一个向量，梯度方向是函数上升最快的方向。沿着梯度下降的方向找到局部最小值，对于凸优化问题，局部最小值就是全局最小值。）<br>Pytorch每求一次误差，梯度累加一次。<br>有了u误差更新梯度，有了梯度更新参数</p>
<p>Python关键字yield生成器：每次调用函数会接着上一次执行的结果执行后返回。和return不同，接着取上一次的数据同时还会把后面的函数执行完毕。取生成器里面的数据要通过for来取。</p>
<p>训练数据可能会存在前1000条是一个类别，后1000是一个类别，然后通过batch取得数据的时候取得的都是一个类型。所以在batch取数据的时候要进行shuffle。训练数据越乱越好，尽量包含多个类别数据。</p>
<p>把每个模块的目的搞清楚，然后自己写。</p>
<p>损失函数，用的较多的是MSE，交叉熵<br>weight_decay权重衰减<br>如果预测值w很大，<br>损失函数正则化：求损失的时候再加一个约束项（关于权重的）</p>
<p>My question<br>【一】batch数据变量化，什么二维切片<br>for i,inst in enumerate(batch_data):<br>    seq_len = len(inst.words)<br>    # 把实际长度覆盖掉<br>    wd_idx[i,:seq_len] = torch.tensor(wd_vocab.word2index(inst.words))<br>    lbl_idx[i] = torch.tensor( wd_vocab.label2index(inst.label))</p>
<p>【二】cpu  gpu<br>模型数据要放在同一个设备上。Train和datalodar里面都要加。Args。Device<br>【三】batch_X,batch_y = batch_variable(batch_data,self._vocab,self._args.device)<br>【四】# 用梯度去更新模型参数</p>
<h1 id="①加动量改变梯度方向；②改变学习速率（先快后慢）；"><a href="#①加动量改变梯度方向；②改变学习速率（先快后慢）；" class="headerlink" title="①加动量改变梯度方向；②改变学习速率（先快后慢）；"></a>①加动量改变梯度方向；②改变学习速率（先快后慢）；</h1><p>【五】weight_decay权重衰减<br>【六】# 计算准确率<br>def _calc_acc(self,pred,target):<br>    # pred dim = batch_size,label_size<br>    torch.eq(torch.argmax(pred,dim=1),target)<br>    pass</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-12/" itemprop="url">2019-7-12</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:46:49+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>【一】黄洪：字符串转换为id，word embedding。<br>预训练的词向量用于embedding。<br>映射：word对应一个id，一个id对应一个词向量。<br>首先预训练出一个词对应向量的词表。<br>然后embedding，pytorch里面。<br>写代码写代码</p>
<p>读取文件，变成string，vocab把word变成数字编号（连续有上界）。<br>Embedding矩阵可以随机初始化；也可以通过外面的词向量预训练得到。<br>【二】杨林：预训练得到词向量（语料库）Embedding怎么来的。<br>fastText提供了集中训练词向量的模型。Fasttext包含了Word2vec。<br>Skipgram模型：找到一个词的上下文信息，也就是语义信息。<br>官方文档参数解释：mincount（规定一个词出现的最小次数，只有达到这个数值的词才可以归到语料库里面）wordNgrams（就是把词比如2gram就是随机把一句话的两个词进行组合。3gram就是随机3个词进行组合。）bucket（和哈希有关系，加快训练查找什么的）minn maxn（规避oov问题）dim（训练的向量维度）lr（learning right在拟合的过程中，首先要在损失函数上面选定一个点，目标是逼近最低点，lr就是长度，每次逼近时移动的长度）loss（损失函数）ws（context window就是怎么定义上下文）epoch（训练多少轮）neg（一般选5）pretrainvector是目标。</p>
<p>【三】吴林志<br>神经网络，首先将输入节点做一个线性或非线性变化，然后激活函数，输出。Y=g(wx+b)。<br>函数就是映射，假设有一个函数f(“cat”)=猫。<br>深度学习最基本的运算就是矩阵。<br>常见激活函数tanh，symoid，relu（公式下去了解）。<br>n多个节点的话：<br>如果每个节点和下一个节点都有连接，就是输入层、隐层、输出层。<br>不全连接，只是部分连接，参数很多，这就是卷积网络（局部连接，权值共享）<br>卷积网络：卷积核（就是输入序列取多少个节点，窗口大小，）<br>卷积操作：加权求和，随着窗口移动对应位置相乘，然后进行卷积结果拼接。</p>
<p>通过窗口局部卷积，卷积一般都是向上取整，填充为了保证输入输出一致。卷积核固定情况下，通过pad做伸缩，二维里面就是保证输出形状基本不变，同时信息提取的更充分。<br>池化层：跟卷积操作一样，不一样的就是对卷积的结果进行处理，一般都是取最大值或者取平均值。<br>经典的卷积：先做一个卷积层，然后做一个线性变化relu（假如卷积信息有效就保留，无用就筛选），最后经过一个池化层（池化层看情况加，大多数情感分析都加池化层。对图像来说，池化目的就是从一堆特征选出一个更好的特征。对于文本来说池化就是得出一个固定的值）。</p>
<p>加激活函数是为了让模型非线性化。不加的话加再多层也只是个线性组合。</p>
<p>总的一个设计思路：<br>1.面向对象、模块化<br>2.设计过程，增量开发<br>3.从整体到局部</p>
<p>Python pytest单元测试：<br>1.py文件以testXXX命名<br>2.方法名也是testXXX</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-11/" itemprop="url">2019-7-11</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:45:54+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>重视实践<br>比如文本分类重要的是情感分析，不能只从书上知道，要实践。</p>
<p><a href="https://nlp.stanford.edu/projects/" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/</a><br>培训两个月之后要交一个技术报告，要写的很详细具体，比如你对attention机制的理解，你觉得深度学习最重要的部分。<br><a href="https://arxiv.org/list/cs.CL/recent" target="_blank" rel="noopener">https://arxiv.org/list/cs.CL/recent</a></p>
<p>要用pytorch<br>大的方向：文本分类任务</p>
<p>下午：<br>【一】黄洪部分：<br>从数据集拿到数据，到扔到模型之前的处理。<br>主要步骤：<br>（1）数据读取加载。<br>可以做分词的nltk,spacy。<br>数据加载：Dataloader类；统计词的频率：Counter类。<br>根据任务进行分类，比如二分类就标记0,1。标签和文本之间要通过符号分割。<br>（2）预处理。<br>①进行符号的统一，比如中英文的标点符号不一样会影响歧义。空格相关的也进行处理。就是把后面的文本规范化。<br>②文本加载，pytorch有一个自带的类可以自动加载（但是是黑盒的，要保证完全理解它的源码，完全掌握可以用，甚至可以自己拓展）。手动写的话比较灵活，可以按照你定义的规则来组成一个instance，就像我们检测代码克隆读取一个一个方法，通过大括号来提取。<br>（3）One-hot编号。Word to Id，词转换成id。<br>Word embedding<br>（4）生成batch（批处理，在机器学习上不是必须的。但是深度学习数据量比较大，）。通过调用pytorch。</p>
<p>【二】张老师：<br>训练、开发、测试，防止以后做开发出现误区：<br>不要怕出错，在错误中学习。<br>在训练集上调参，不是在测试集上调参。<br>神经网络有很多不好的地方，随机种子换一下结果就会很不一样。数据标的话一致性比较差，训练样本比较差。所以要换几个随机种子，最后取结果的平均值。<br>或者是交叉验证大参数不需要调，小参数比如迭代次数多次调换。<br>预训练可以规避随机种子的问题，但是预训练和真正训练的数据集是不一样的。<br>传统机器学习方法（在深度学习之前）：<br>Vocab结构就是把字符串转换为数字，字符串可以是word、lable。<br>“我 今天 早上 起 得 很  早”<br>在深度学习之前：每个词一个编号，对应一个10000维的向量（one-hot向量，非常稀疏，只有一个部分是1，其它部分是0）。<br>然后情感分析o=w*x（把一句话转换为一个10000维的向量，哪一位有词就是1没有就是0 ）<br>主要是训练w参数，模型就是参数。<br>深度学习：Word embedding</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/29/transformer/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/29/transformer/transformer/" itemprop="url">Transformer初步了解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-29T16:51:18+08:00">
                2019-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transformer/" itemprop="url" rel="index">
                    <span itemprop="name">transformer</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且<strong>仅由self-Attenion和Feed Forward Neural Network组成</strong>。<br>采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：<br>       1.时间片 t 的计算依赖 t-1 时刻的计算结果，这样限制了模型的并行能力；<br>       2.顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。<br>Transformer的提出解决了上面两个问题，<strong>首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量</strong>；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。<br><img src="https://img-blog.csdnimg.cn/20190929161612774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如论文中所设置的，编码器由6个编码block组成，同样解码器是6个解码block组成。与所有的生成模型相同的是，编码器的输出会作为解码器的输入，如图3所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190929161631536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929161647206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Decoder的结构如图5所示，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：<br>       1. <strong>Self-Attention：当前翻译和已经翻译的前文之间的关系；</strong><br>        2. Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。<br><img src="https://img-blog.csdnimg.cn/20190929161703777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="输入编码"><a href="#输入编码" class="headerlink" title="输入编码:"></a>输入编码:</h2><p><strong>Word2Vec</strong>等词嵌入方法将输入语料转化成特征向量，论文中使用的词嵌入的维度为d=512;<br><img src="https://img-blog.csdnimg.cn/20190929161713284.png" alt="在这里插入图片描述"><br>在最底层的block中， x 将直接作为Transformer的输入，而在其他层中，输入则是上一个block的输出。<br><img src="https://img-blog.csdnimg.cn/20190929161726274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-Attention是Transformer最核心的内容，<br>在self-attention中，<strong>每个单词有3个不同的向量</strong>，它们分别是Query向量(Q),Key向量(k)和Value向量(v),，长度均是64。它们是通过3个不同的权值矩阵由嵌入向量X(1<em>512)乘以三个不同的权值矩阵WQ,WK,WV(512</em>64)得到的。<br><img src="https://img-blog.csdnimg.cn/20190929161736488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>那么Query，Key，Value是什么意思呢？它们在Attention的计算中扮演着什么角色呢？我们先看一下Attention的计算方法，整个过程可以分成7步：<br><img src="https://img-blog.csdnimg.cn/2019092916154161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上面步骤的可以表示为图10的形式。<br><img src="https://img-blog.csdnimg.cn/20190929161954345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>实际计算过程中是采用基于矩阵的计算方式，那么论文中的 Q,K,V的计算方式如图11：<br><img src="https://img-blog.csdnimg.cn/20190929162256228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929162320528.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>在self-attention需要强调的最后一点是其采用了<strong>残差网络 中的short-cut结构</strong>，目的当然是解决深度学习中的退化问题，得到的最终结果如图13。<br><img src="https://img-blog.csdnimg.cn/20190929162440763.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929163721155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929163806782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>同self-attention一样，multi-head attention也加入了short-cut机制。</p>
<h2 id="Encoder-Decoder-Attention"><a href="#Encoder-Decoder-Attention" class="headerlink" title="Encoder-Decoder Attention"></a>Encoder-Decoder Attention</h2><p>在解码器中，Transformer block比编码器中多了个encoder-cecoder attention。在encoder-decoder attention中， Q 来自与解码器的上一个输出， K 和 V则来自于与编码器的输出。其计算方式完全和图10的过程相同。</p>
<p>由于在机器翻译中，解码过程是一个顺序操作的过程，也就是当解码第 k 个特征向量时，我们只能看到第 k-1 及其之前的解码结果，论文中把这种情况下的multi-head attention叫做masked multi-head attention。</p>
<h2 id="损失层"><a href="#损失层" class="headerlink" title="损失层"></a>损失层</h2><p>解码器解码之后，解码的特征向量经过一层激活函数为softmax的全连接层之后得到反映每个单词概率的输出向量。此时我们便可以通过CTC等损失函数训练模型了。</p>
<p>而一个完整可训练的网络结构便是encoder和decoder的堆叠（各 N个， N=6），我们可以得到图15中的完整的Transformer的结构（即论文中的图1）<br><img src="https://img-blog.csdnimg.cn/20190929164330695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>截止目前为止，我们介绍的Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。</p>
<p>为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。</p>
<p>那么怎么编码这个位置信息呢？常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。那么这个位置编码该是什么样子呢？通常位置编码是一个长度为 d(512维) 的特征向量，这样<strong>便于和词向量进行单位加</strong>的操作，如图16。<br><img src="https://img-blog.csdnimg.cn/2019092916470993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190929164836701.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>优点</strong>：（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。（2）Transformer的设计最大的带来性能提升的关键是将<strong>任意</strong>两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</p>
<p><strong>缺点</strong>：（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉<strong>局部特征</strong>的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。（2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/29/网易云学习/Attention机制/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/29/网易云学习/Attention机制/" itemprop="url">Attention机制</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-29T11:09:41+08:00">
                2019-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nn-basic/" itemprop="url" rel="index">
                    <span itemprop="name">nn basic</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Attention机制其实就是一系列注意力分配系数，也就是一系列权重参数罢了。</p>
<p>注意力模型就是要从序列中学习到每一个元素的重要程度，然后按重要程度将元素合并。 </p>
<p>换一个角度来理解，我们将attention机制看做软寻址。就是说序列中每一个元素都由key(地址)和value(元素)数据对存储在存储器里，当有query=key的查询时，需要取出元素的value值(也即query查询的attention值)，与传统的寻址不一样，它不是按照地址取出值的，它是通过计算key与query的相似度来完成寻址。这就是所谓的软寻址，它可能会把所有地址(key)的值(value)取出来，上步计算出的相似度决定了取出来值的重要程度，然后按重要程度合并value值得到attention值，此处的合并指的是加权求和。</p>
<p>优点:<br>一步到位的全局联系捕捉</p>
<p>上文说了一些，attention机制可以灵活的捕捉全局和局部的联系，而且是一步到位的。另一方面从attention函数就可以看出来，它先是进行序列的每一个元素与其他元素的对比，在这个过程中每一个元素间的距离都是一，因此它比时间序列RNNs的一步步递推得到长期依赖关系好的多，越长的序列RNNs捕捉长期依赖关系就越弱。<br>并行计算减少模型训练时间</p>
<p>Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理。但是CNN也只是每次捕捉局部信息，通过层叠来获取全局的联系增强视野。</p>
<p>缺点:<br>缺点很明显，attention机制不是一个”distance-aware”的，它不能捕捉语序顺序(这里是语序哦，就是元素的顺序)。这在NLP中是比较糟糕的，自然语言的语序是包含太多的信息。如果确实了这方面的信息，结果往往会是打折扣的。说到底，attention机制就是一个精致的”词袋”模型。所以有时候我就在想，在NLP任务中，我把分词向量化后计算一波TF-IDF是不是会和这种attention机制取得一样的效果呢? 当然这个缺点也好搞定，我在添加位置信息就好了。所以就有了 position-embedding(位置向量)的概念了，这里就不细说了。</p>
<p>Attention机制说大了就一句话，分配权重系数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/head.gif" alt="Chengxiaoyun">
            
              <p class="site-author-name" itemprop="name">Chengxiaoyun</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chengxiaoyun</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
