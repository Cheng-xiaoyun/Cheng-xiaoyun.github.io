<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="丑小鸭的栖息地">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="丑小鸭的栖息地">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="丑小鸭的栖息地">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>丑小鸭的栖息地</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">丑小鸭的栖息地</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/21/skillness/在服务器中配置anaconda以及pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/21/skillness/在服务器中配置anaconda以及pytorch/" itemprop="url">在服务器中配置anaconda以及pytorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-21T19:55:12+08:00">
                2019-10-21
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/skilledness/" itemprop="url" rel="index">
                    <span itemprop="name">skilledness</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>安装anaconda</p>
<p>（1）在官网下载对应的Anaconda3-2019.07-Linux-x86_64.sh到你想安装的目录。</p>
<p>（2）输入命令 bash Anaconda3-2019.07-Linux-x86_64.sh。</p>
<p>（3）vi .bashrc   将路径添加到.bashrc最后</p>
<pre><code>（比如export PATH=$PATH:/home/chengxiaoyun/anaconda3/bin）</code></pre><p>（4）source .bashrc 激活配置文件</p>
<p>conda配置pytorch</p>
<p>（1）在pytorch的官网找到你要下载的版本的对应命令。（注意，如果是gpu服务器要查找自己服务器的cuda版本，命令是cat /usr/local/cuda/version.txt）</p>
<p>（2）创建虚拟环境conda create -n pytorch pthon=3.7（我这个是在python3.7下创建的一个叫做pytorch的环境）</p>
<p>（3）source activate pytorch 激活环境</p>
<p>（4）将第（1） 步查找的命令输入。比如我的是</p>
<pre><code>conda install pytorch torchvision cudatoolkit=10.1 -c pytorch</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/19/实验记录/P值和Recall值/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/19/实验记录/P值和Recall值/" itemprop="url">P值和Recall值</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-19T19:15:26+08:00">
                2019-10-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/实验记录/" itemprop="url" rel="index">
                    <span itemprop="name">实验记录</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="https://img-blog.csdnimg.cn/20191019191455789.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDA4NjczNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>就比如我们的这组实验，其中clone是type1-5相加，没有把type6计算在内。p值说明是 （你查出来正确的/你总共查出来的），recall是 （你查出来正确的/总共正确的），noclone的recall低说明没有把全部的noclone检测出来，那没有检测出的那部分noclone被我们误判为了clone，所以在clone的prec值就会降低，因为我们查找的clone里面有很大一部分是noclone<br><img src="https://img-blog.csdnimg.cn/20191019191511990.png" alt="在这里插入图片描述"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/17/transformer/transformer的encoder部分/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/17/transformer/transformer的encoder部分/" itemprop="url">transformer的encoder部分</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-17T11:40:52+08:00">
                2019-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transformer/" itemprop="url" rel="index">
                    <span itemprop="name">transformer</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>6个encoder结构相同，参数不同。<br>self-attention：两个相同序列直接的atention。<br>两个不同序列之间的atteion就是一个标准的attention。</p>
<p>decodee部分：对ecoder和decoder序列之间做了一个attention。</p>
<p>具体什么是attention：比如输入是一个序列“我要吃饭”经过一个双向的rnn，对每个时刻正向和反向进行一个拼接，拿到每个时刻对应的embedding向量，以前的话只拿最后一个时刻和上一个时刻的隐层状态共同预测这个时刻。<br>attention考虑所有时刻<br>attetion是根据你给的信息去查之前的所有信息。查询引擎对每个时刻都算一下相似度，做一下归一化，做一下加权，拿到加权平均的值，然后去预测下一个。重点是权重，可以通过点乘。</p>
<p>atention（Q,K,V）<br>怎么理解Q K V，就比如网页，Q是要查的词 K是你的url地址，V是网页的内容。<br>对应网络，Q是上一个时刻的输出，要查的词，在整个序列里查 K是当前时刻对应的编码，双向lstm隐状态的拼接， V也是隐状态的拼接，K V是一样的。<br>先算Q K的相似度，做一下softmax，再对V进行加权求和，就能拿到attention的值。</p>
<p>self-attention的Q K V来自同一个序列。</p>
<p>每一个时刻对应的特征是定长的。</p>
<p>self-attention的计算：</p>
<p>（具体的例子，序列就有两个词：thinking，machines）<br><img src="https://i.imgur.com/3pizv2X.png" alt><br>Q是x1<em>wq就是Q，也就是进行了一个线性变化。（1，4）</em>（4，3）得到（1，3）的矩阵。K v 也是这样来的。w（q，k，v）最开始就是随机初始化的，再后来不断训练就得到好的w。</p>
<p>计算self-attention：<br><img src="https://i.imgur.com/9J9RxGD.png" alt><br>首先计算q和k的相似度。<br>先计算x1对应的所有的向量，要通过q1去查，q1怎么查呢，通过和k1和k2进行比较。q1和k1进行相乘，q1和k2进行相乘，因为序列是两个词，每个词都会对应一个value，一个k，所以要让所有k和q1相乘。乘完了之后进行根号或者除法，然后进行softmax，softmax就是为了变成一个概率值，就是相加等于一。也就是图中的0.88+0.12=1<br>然后进行加权求和，softmax已经求出了权重，也就是说q1和第一个时刻thinking对应的权重是0.88，和第二个时刻machine对应的权重是0.12。</p>
<p>接着加权求和，权重和每个时刻相乘然后相加 0.88×v1+0.12×v2=z1。</p>
<p>也就是说想拿到第一个时刻的值，就是拿第一个时刻对应的q和所有时刻的k进行一下相乘经过softmax得到权重，然后拿着权重和每个时刻的v进行相乘后求和，拿到所有的信息进行一下累计和，就能拿到z1。<br>q2同理。</p>
<p>可能有多个attention，每个attention关注点不一样，一个x乘多个wq生成多个q，这就是muli-head attention。多头就能获取到多个信息。</p>
<p>Encoder的主要工作：</p>
<p>将输入的序列，经过attention拿到z1，z2，…self-attention拿到每个时刻是等长的，然后经过一个全连接网络（feed forward），就是对每一个时刻连接，每个时刻对应网络拿到的参数是一样的，拿到参数可以输出每个时刻的序列。</p>
<p>transformer的encoder输入的是序列，输出的也是序列，就是将拿到的序列学到了更好的方法。将序列累加就能得到第二个encoder。依次。</p>
<p>每个时刻（每个单词）都要经过全连接网络，参数是共享的。</p>
<p>原始论文加了一个残差网络：</p>
<p><img src="https://i.imgur.com/X7icyeS.png" alt></p>
<p>layernorml是一种归一化的方法，保证梯度有效的学习。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/17/实验记录/重新生成type3语料进行实验/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/17/实验记录/重新生成type3语料进行实验/" itemprop="url">重新生成type3语料进行实验</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-17T11:39:36+08:00">
                2019-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/实验记录/" itemprop="url" rel="index">
                    <span itemprop="name">实验记录</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>重新生成10000对MT3的数据<br>存在的问题：</p>
<p>之前的论文中提到我们自动生成的语料在MT3上效果比不上人工标注的，我们给出的解释是MT3的相似度在0.5-0.7之间，我们通过exchange和insert block块相似度没有控制。<br>初步解决：<br>（1）限制相似度在0.5-0.7之间<br>①限制exchange的type3就是通过（L1+L3+L5）/(L1+L2+L3+L4+L5)在0.5-0.7中间，也就是没有变化的代码除以原来代码长度。<br>②限制插入block块的type3同理，就是（原来代码长度）/（block块长度+原来代码长度）。但是发现存在问题，就是我们的block块平均在1-10之间，但是我们的代码行数都很大，20的和200行的都有，所以得先重新生成block块。<br>重新生成的block块长度在12-30之间（我是按照平均代码行数30计算的，没有百分百的合理，但是足够生成我们想要的数据集）。<br>（2）重新跑网络<br>在跑网络的时候发现效果很不好，然后发现还是自动生成的数据集有问题。<br>在杨林的代码generate-train-clone中type1的生成是通过把生成的type3里面不重复的函数拿出来生成type1。<br>这样可能存在的问题就是，我再type3生成时依赖的是高毅学长的ant3_t3_bench.csv，依次读取这里面的函数，针对每个函数进行exchange、insert操作之后与它本身结合成一对。 但是ant3_t3_bench.csv这里面会有重复的函数，虽然不影响type3的生成（因为即使对同一个函数做两次exchange操作结果一样的概率很低很低），但是依赖这些type3的函数再次生成type1除去重复的函数，数据量就不好控制了。<br>然后修改了一下代码，重新生成了10000对的type3数据，都限制在0.5-0.7中间，同时根据这10000对生成10000的type1、2，no-clone还是根据原来的代码生成了10000对。这样重新组成了30000的数据集，再和我们原来的数据集合并，重新跑网络。<br>（3）结果分析<br>最后的结果见文件“合并-MT3-Test”，可以看到type3、4、5都有明显的提升，说明我们控制type3的相似度是有效果的。至于准确率下降，可能是因为我们合并的两组数据的no-clones是一样的，也就是有20000重复的no-clones数据集，所以才会P下降那么多。</p>
<p>接下来就要再生成10000不同的no-clones再次进行实验。<br>（4）将来工作<br>如果MT3的效果好的话，就可以按照一样的思路来控制VT3、ST3、WT3的生成。首先把自动生成的数据集的质量提高上去，然后再跑transformer。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/11/skillness/python文件读写模式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/11/skillness/python文件读写模式/" itemprop="url">python文件读写模式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-11T14:36:26+08:00">
                2019-10-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/skilledness/" itemprop="url" rel="index">
                    <span itemprop="name">skilledness</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>python文件读写模式<br>r : 读取文件，若文件不存在则会报错</p>
<p>w: 写入文件，若文件不存在则会先创建再写入，会覆盖原文件</p>
<p>a : 写入文件，若文件不存在则会先创建再写入，但不会覆盖原文件，而是追加在文件末尾</p>
<p>rb,wb：分别于r,w类似，但是用于读写二进制文件</p>
<p>r+ : 可读、可写，文件不存在也会报错，写操作时会覆盖</p>
<p>w+ : 可读，可写，文件不存在先创建，会覆盖</p>
<p>a+ ：可读、可写，文件不存在先创建，不会覆盖，追加在末尾</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-24/" itemprop="url">2019-7-24</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:49:47+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>经典序列标注任务<br>自然语言任务：分类任务、结果学习任务。<br>词性标注：我（名词） 家 养（动词） 了 只（量词） 猫。标签之间也是有依赖关系的，比如名词后面经常跟动词。</p>
<p>【分词】<br>中文、日语词语词之间是没有间隔的。把分词转换为序列标注问题<br>例子一：“天津大学新校区位于津南区” 进行分词<br>几种分词标签：①Single就是单个标签，比如“猫”②Begin比如“天津大学”③Middle“津大”④End。<br>“小 明 家 养 了 一 只 猫”<br>“B  E  S  B  E  B  E  S”<br>标签之间有一些依赖和限制，比如S不可能接M或者E。<br>通常是在LSTM接一个CRF用来解决标签之间相互转换为关系。比如要对“天津大学新校区位于津南区”进行分词，CRF模型对词进行打分，比如“南”这个字被预测为B E M S的概率。对于“津”的标签预测是和“南”字有关联的，所以在对“津”打分的时候，分为两部分，第一部分是这个字被预测为B E M S的概率，另一部分是前一个词“南”被。<br>求解空间是4的n次方，n是这句话有几个字，每个词被预测为4种标签的概率。假如每个字之间没有任何依赖关系，想得到最好的预测，最优子结果不仅包括每个词的预测，还有每个词之间的联系也就是边的值。<br>怎么做训练：</p>
<p>【补充】<br>Shape属性。查看矩阵或者数组的维数，shape[0] 为第一维的长度，shape[1] 为第二维的长度<br>Reshape()函数重新定义了原张量的阶数：给数组一个新的形状而不改变其数据。通过reshape生成的新数组和原始数组公用一个内存，也就是说，假如更改一个数组的元素，另一个数组也将发生改变 。</p>
<p>a=np.array([1,2,3,4,5,6,7,8,9,10,11,12])<br>b=np.reshape(a,(2,-1))<br>c=np.reshape(a,(2,2,-1))<br>d=np.reshape(a,(2,3,-1))<br>b=<br>[[ 1  2  3  4  5  6]<br> [ 7  8  9  10 11 12]]<br>c=<br>[[[ 1  2  3]<br>  [ 4  5  6]]</p>
<p> [[ 7  8  9]<br>  [10 11 12]]]<br>d=<br>[[[ 1  2]<br>  [ 3  4]<br>  [ 5  6]]</p>
<p> [[ 7  8]<br>  [ 9 10]<br>  [11 12]]]</p>
<p>Squeeze<br>Unsqueeze<br>Size（）</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-23/" itemprop="url">2019-7-23</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:49:17+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>常见文本关系：<br>文本蕴涵（有方向性，侧重在语义，A推出B，B退不出A）、文本相似（文本字面结构上的相似，不是说语义）、文本复述（文本蕴涵的一种特殊情况，A能推出B ，B能推出A）、逻辑推理（侧重在逻辑方面，文本蕴涵侧重在语义方面）</p>
<p>自然语言的一些任务，任务之一文本蕴涵：<br>【一】文本蕴涵（判断两个句子）三种叫法<br>STS semantic sentence similarity 判断两个句子语义相似性<br>RTE 判断两个句子，一个包含另外一个<br>NLI 两个句子，一个前提一个假设，能否通过一个推出另外一个<br>【二】文本蕴涵的三种标签<br>Entailment蕴含关系：意思相同，A推出B，<br>Neural 中性关系：没有任何关系<br>Contradiction反对关系：意义相反，矛盾<br>【三】文本蕴涵应用场景<br>①问答<br>②信息检索<br>③关系识别<br>④知识获取<br>⑤蕴含对生成<br>【四】文本蕴涵任务里面公开的数据集（都是英文的）<br>MSRP 从一些新闻文章中提取的数据集，里面有很多复杂的语义。<br>Quora 本质上是一个知识分享网站，所有的句子都是一个问题，然后判断这两个问题的关系。<br>SICK   在一个网站上发表一些任务，然后各界人来做标注。<br>SNLI   人工书写的英语子对集合。数据集很大。<br>Scitall 有一个问题，多个选项，选择一个正确答案，然后转换为断言陈述。<br>【五】方法<br>TF-IDF 判断两句话的相关性<br>ESIM模型：有两种（双向LSTM、基于树的LSTM）<br>【六】特征<br>词频特征、词嵌入特征</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-22/" itemprop="url">2019-7-22</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:48:57+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>张老师：<br>【一】<br>Embedding就是一个表，包含词和索引。<br>每次查表，找对应词的向量表示，就是一个operation。<br>不管函数多复杂，总是可以通过分解为一些原子操作，比如查表、线性变化、<br>三种基本运算：<br>①Embedding<br>②Wx<br>③w——x——h——RNN（pooling）——hp——wx（分类、线性变化）<br>整个预测，本质上就是一个函数MLP—pooling—BMLP—MLP—embedding。把这个函数打开，就像是一个图，按照图的拓扑顺序。<br>深度学习就是一个复合函数，为每个原子（节点）定义一个input——forward——output——backward——input。<br>整个神经网络就是模仿一个复合函数。<br>机器学习怎么来：<br>目标函数怎么定义：交叉熵。熵越大损失越大。<br>优化损失值：梯度下降。要把损失往回传，<br>定义好MLP的偏导，就是backward，然后沿着拓扑反方向一层一层往前求。复合函数求导就是深度学习的根基。<br>如果自己不写网络都不需要管backward。<br>Forward：Y=wx+b<br>Backward：求导</p>
<p>LSTM、RNN不能并行计算因为每一步都与前面有关联，但是Transform可以并行计算。<br>Op的概念：每个op都有forward、backward。<br>拓扑的概念：每个句子是一棵树，但是多个句子就是图，并行一定要高。<br>优化的概念：不搞机器学习都不用管。</p>
<p>哪些工作取决于编码能力。<br>【二】<br>性能好：①模型本身；②输入有多丰富。<br>词向量获取Pre trainning<br>语言模型任务：通过“我 喜欢 * 苹果”预测“吃”这个词。这样的语料无穷无尽，任何一个合理的句子都会提供一个语料。选取高频的词。最简单的词向量模型CBow，就是一个分类问题，难点就是预测。预训练的语调要够大。<br>Skipgram正好相反，用“吃”这个词预测它的上下文。</p>
<p>Deep bilstm：情感分析，词性标注，实体识别，句法分析，文本蕴涵，……<br>语言模型：几乎无限的预训练语料。</p>
<p>Language Model<br>ELMO目前最好用的词向量。<br>BERT（transform代替lstm）<br>改进：①Lstm有方向，transform没有方向。②③超大规模数据测。④精细的调参技术。<br>吴林志<br>【三】Char Embedding<br>把字符特征加到 表示信息更丰富。拼接字符级特征。<br>字符词向量没有预训练好的表，都是随机初始化的。<br>训练语调统计的字符不包含全部，所以把字符表建立的完备一点。<br>如果embedding_dim是300维度 char_embed_dim是200维度，那最后拼接就是500 维度，包含了字符特征。</p>
<p>标准正太分布，方差<br> 初始化可以用xavier_normal，会根据前一层节点，当前一层有n个节点的时候，根据标准差为根号1/n进行正态分布初始化，当激活函数tanh、s型曲线的时候用这个<br> 初始化也可以用kaiming_normal，对xavier_normal的改进，标准差乘2进行正态分布，当激活函数relu的时候用这个<br>一般用kaiming_normal，因为大多数激活函数是relu</p>
<p>shape是一个属性，size是方法。</p>
<p>Python  train – char_hidden_size 150 –char_embed_dim 50<br>【问题1】<br>每个索引如果是3维度，经过embedding就变成4维<br>【问题2】<br>conv_out =torch.cat( [self._convs(embed) for _convs in self._convs],dim=1).squeeze(dim=2)<br>【问题3】<br>out.reshape(batch_size, max_seq_len ,-1)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-19/" itemprop="url">2019-7-19</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:48:12+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Transformer<br>Seq——seq机器翻译模型<br>中文——encoder编码——decorder解码——英文<br>Attention只是局部提取信息，每个信息之间没有连接起来。循环结构不需要加位置信息，因为序列循环已经包含了位置信息。但是卷积就需要位置信息，序列的先后顺序，因为他学习不到位置信息。可以给每个节点加一个位置信息，positional encoding。</p>
<p>Attention注意力机制：<br>不是一种模型，而是一种思想。有翻译就一定要用到attention。<br>“我喜欢恐怖片”是正面的，因为人的大脑关注的是局部信息，人们注意到的是“喜欢”这个词。深度学习也是从大量信息中选择需要的重要信息。之前的cnn rnn只学习到“喜欢”这个局部特征，没有考虑情感词。怎么体现这个选择性呢，通过权重，当遇到“喜欢”这个词的时候就权重分配大一点。<br>给序列分配权重：<br>通过点乘，或者余弦，去找到下一步的权重。找到权重之后加权求和。</p>
<p>这是一个模型，没有具体含义，在具体问题中才有含义。<br>用一个外部的查询变量，查询序列Q K的相似度（相似度也就是和哪个关联最大），越相似的权重越高，然后施加到V上面。除以根号dk是防止过大。相似度同时除以一个值，不影响相似度。<br>【相似性：从英文（Q）——attention——翻译为法文(K)<br>从历史隐层信息，根据已经翻译的结果，去找序列里面哪个相似度最高的来找下一个，Q里面有学好的上下文历史信息，帮助预测下一步，权重怎么分配是根据已经学好的历史信息】<br>用一个外部的查询变量，先通过点乘找相似度，通过softmax概率化归一化变成概率，使得和为1.然后施加到V上面求和。</p>
<p>Lstm没有学到局部信息，只学习到了序列信息。<br>Self attention：<br>是Q没有用到外部信息，只用到了自身信息。Q=K=V。只寻找序列内部的联系。<br>（I like apple and it is dilicious自己学习，网络把dilicious指向apple。）无视词之间的距离，直接计算距离关系，能够学习到一个句子的内部结构。直接找，不需要从0时刻开始把前面的信息累计起来，比如dilicious直接找apple。卷积就是直接卷积，没有迭代，不需要把前面执行完，比lstm序列模型速度快。<br>点乘可以并行。矩阵乘不能做并行（因为矩阵是行元素乘列元素累加）。能否做并行就看他的局部变量是否是独立的。<br>残差连接：<br>是<br>Encoder编码：<br>是</p>
<p>Maxpooling本身就是一个attention，取最大元素，就是注意力。<br>要用attention需要外部辅助信息，很多时候没有外部信息，更多时候是用自己的信息去产生权重，也就是self attention，attention重点就是权重。方法就是利用末状态的隐藏层信息h——next，h和c都可以，h就是对c做了线性变化，h涵盖了c。</p>
<p>采用attention依旧存在的问题：<br>（1）序列问题存在一个对齐问题，有的序列是做了填充padding的，有可能最后几个元素是填充的，但是也被分了权重，使得原来有元素的位置权重被分了。<br>解决办法：mask<br>在softmax的时候分配权值，控制padding填充部分不分配权值。把padding部分传入负无穷。</p>
<p>使用服务器设置参数–cuda 1</p>
<p>My question：<br>【1】mask，传入负无穷，就可以分配权值为0。-mask就是padding部分，然后填充为负无穷。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-17/" itemprop="url">2019-7-17</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:47:48+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。<br>循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>
<p>捕捉序列信息，循环神经网络，常见的有LSTM，GRU（lstm的变体），RNN。<br>把之前的信息不断累积，通过一个迭代结构，把之前的序列信息，加上当前时刻的信息。历史信息都学到了。加了一个循环的结构。<br>与传统不同的是：<br>引入了一个新的状态累计信息，引入了一个门机制。<br>可能存在的问题：<br>训练时更新权重：梯度爆炸 梯度离散<br>小于0的数越乘越小，或者是越来越大。两种极端。<br>出现这种异常梯度的原因是序列太长的话，就只能学习到几步，梯度障碍。再往前追溯追溯不了。</p>
<p>几种门机制：输入门，输出门，遗忘门（控制前一时刻多少信息流入）</p>
<p>底层的网络能捕获基本信息，层数加深就能捕获到语义信息。双向网络（有的信息是后缀的，必须从两个方向去捕捉语义，比如“衣服 差 的 不行”。BiLSTM）</p>
<p>reverse()是一个list的反转。reversed是一个内置的反转。</p>
<p>Question：<br>【1】什么元祖什么的</p>
<h1 id="lstmcell-grucell-fw-next-不是元祖-h1-c1迭代"><a href="#lstmcell-grucell-fw-next-不是元祖-h1-c1迭代" class="headerlink" title="lstmcell/grucell   fw_next 不是元祖 h1=c1迭代"></a>lstmcell/grucell   fw_next 不是元祖 h1=c1迭代</h1><p>for xi in range(seq_len):<br>    if self._rnn_type == ‘LSTM’:<br>        # lstmcell   fw_next也是元祖h1 c1  h0 c0<br>        h_next,c_next = self._rnn_cell(inputs[xi],fw_next)<br>        # 先拆开后合并，做一个过滤<br>        h_next = h_next * mask[xi]<br>        c_next = c_next * mask[xi]<br>        fw_next = (h_next,c_next)<br>        outputs.append(h_next)<br>    else:<br>      # RNNcell/GRUcell   fw_next 不是元祖 h1=c1迭代<br>      fw_next = self._rnn_cell(inputs[xi], fw_next)<br>      fw_next = fw_next * mask[xi]<br>      outputs.append(fw_next)</p>
<p>【2】掩码填充什么的<br>【3】def _forward(self,inputs,init_hidden,mask):<br>    ‘’’<br>    :param inputs: inputs[seq_len,batch_size,input_size]<br>    :param init_hidden:[batch_size,hidden_size],如果是lstm，则init_hidden类型是元祖<br>    :param mask: inputs[seq_len,batch_size,hidden_size]<br>    :return:<br>    ‘’’<br>也可以这样处理mask  h_next = h_next * mask[xi]+init_hidden[0]<em>(1-mask[xi])<br>【4】#  outputs保留的是中间状态，每个大小都是batch_size</em>hidden_size<br>【5】为什么要返回三维的torch.stack(tuple(outputs),dim=0)<br>【6】bilstm和lstm  rnn的关系，为什么在bilstm里面调用rnn</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/head.gif" alt="Chengxiaoyun">
            
              <p class="site-author-name" itemprop="name">Chengxiaoyun</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chengxiaoyun</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
