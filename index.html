<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="丑小鸭的栖息地">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="丑小鸭的栖息地">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="丑小鸭的栖息地">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>丑小鸭的栖息地</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">丑小鸭的栖息地</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/17/transformer/transformer的encoder部分/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/17/transformer/transformer的encoder部分/" itemprop="url">transformer的encoder部分</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-17T11:40:52+08:00">
                2019-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/transforme/" itemprop="url" rel="index">
                    <span itemprop="name">transforme</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>6个encoder结构相同，参数不同。<br>self-attention：两个相同序列直接的atention。<br>两个不同序列之间的atteion就是一个标准的attention。</p>
<p>decodee部分：对ecoder和decoder序列之间做了一个attention。</p>
<p>具体什么是attention：比如输入是一个序列“我要吃饭”经过一个双向的rnn，对每个时刻正向和反向进行一个拼接，拿到每个时刻对应的embedding向量，以前的话只拿最后一个时刻和上一个时刻的隐层状态共同预测这个时刻。<br>attention考虑所有时刻<br>attetion是根据你给的信息去查之前的所有信息。查询引擎对每个时刻都算一下相似度，做一下归一化，做一下加权，拿到加权平均的值，然后去预测下一个。重点是权重，可以通过点乘。</p>
<p>atention（Q,K,V）<br>怎么理解Q K V，就比如网页，Q是要查的词 K是你的url地址，V是网页的内容。<br>对应网络，Q是上一个时刻的输出，要查的词，在整个序列里查 K是当前时刻对应的编码，双向lstm隐状态的拼接， V也是隐状态的拼接，K V是一样的。<br>先算Q K的相似度，做一下softmax，再对V进行加权求和，就能拿到attention的值。</p>
<p>self-attention的Q K V来自同一个序列。</p>
<p>每一个时刻对应的特征是定长的。</p>
<p>self-attention的计算：</p>
<p>（具体的例子，序列就有两个词：thinking，machines）<br><img src="https://i.imgur.com/3pizv2X.png" alt><br>Q是x1<em>wq就是Q，也就是进行了一个线性变化。（1，4）</em>（4，3）得到（1，3）的矩阵。K v 也是这样来的。w（q，k，v）最开始就是随机初始化的，再后来不断训练就得到好的w。</p>
<p>计算self-attention：<br><img src="https://i.imgur.com/9J9RxGD.png" alt><br>首先计算q和k的相似度。<br>先计算x1对应的所有的向量，要通过q1去查，q1怎么查呢，通过和k1和k2进行比较。q1和k1进行相乘，q1和k2进行相乘，因为序列是两个词，每个词都会对应一个value，一个k，所以要让所有k和q1相乘。乘完了之后进行根号或者除法，然后进行softmax，softmax就是为了变成一个概率值，就是相加等于一。也就是图中的0.88+0.12=1<br>然后进行加权求和，softmax已经求出了权重，也就是说q1和第一个时刻thinking对应的权重是0.88，和第二个时刻machine对应的权重是0.12。</p>
<p>接着加权求和，权重和每个时刻相乘然后相加 0.88×v1+0.12×v2=z1。</p>
<p>也就是说想拿到第一个时刻的值，就是拿第一个时刻对应的q和所有时刻的k进行一下相乘经过softmax得到权重，然后拿着权重和每个时刻的v进行相乘后求和，拿到所有的信息进行一下累计和，就能拿到z1。<br>q2同理。</p>
<p>可能有多个attention，每个attention关注点不一样，一个x乘多个wq生成多个q，这就是muli-head attention。多头就能获取到多个信息。</p>
<p>Encoder的主要工作：</p>
<p>将输入的序列，经过attention拿到z1，z2，…self-attention拿到每个时刻是等长的，然后经过一个全连接网络（feed forward），就是对每一个时刻连接，每个时刻对应网络拿到的参数是一样的，拿到参数可以输出每个时刻的序列。</p>
<p>transformer的encoder输入的是序列，输出的也是序列，就是将拿到的序列学到了更好的方法。将序列累加就能得到第二个encoder。依次。</p>
<p>每个时刻（每个单词）都要经过全连接网络，参数是共享的。</p>
<p>原始论文加了一个残差网络：</p>
<p><img src="https://i.imgur.com/X7icyeS.png" alt></p>
<p>layernorml是一种归一化的方法，保证梯度有效的学习。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/17/实验记录/重新生成type3语料进行实验/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/17/实验记录/重新生成type3语料进行实验/" itemprop="url">重新生成type3语料进行实验</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-17T11:39:36+08:00">
                2019-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/实验记录/" itemprop="url" rel="index">
                    <span itemprop="name">实验记录</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>存在的问题：<br>之前的论文中提到我们自动生成的语料在MT3上效果比不上人工标注的，我们给出的解释是MT3的相似度在0.5-0.7之间，我们通过exchange和insert block块相似度没有控制。</p>
<p>初步解决：</p>
<p>（1）限制相似度在0.5-0.7之间</p>
<p>①限制exchange的type3就是通过（L1+L3+L5）/(L1+L2+L3+L4+L5)在0.5-0.7中间，也就是没有变化的代码除以原来代码长度。</p>
<p>②限制插入block块的type3同理，就是（原来代码长度）/（block块长度+原来代码长度）。但是发现存在问题，就是我们的block块平均在1-10之间，但是我们的代码行数都很大，20的和200行的都有，所以得先重新生成block块。</p>
<p>重新生成的block块长度在12-30之间（我是按照平均代码行数30计算的，没有百分百的合理，但是足够生成我们想要的数据集）。</p>
<p>（2）重新跑网络</p>
<p>在跑网络的时候发现效果很不好，然后发现还是自动生成的数据集有问题。<br>在杨林的代码generate-train-clone中type1的生成是通过把生成的type3里面不重复的函数拿出来生成type1。</p>
<p>这样可能存在的问题就是，我再type3生成时依赖的是高毅学长的ant3_t3_bench.csv，依次读取这里面的函数，针对每个函数进行exchange、insert操作之后与它本身结合成一对。 但是ant3_t3_bench.csv这里面会有重复的函数，虽然不影响type3的生成（因为即使对同一个函数做两次exchange操作结果一样的概率很低很低），但是依赖这些type3的函数再次生成type1除去重复的函数，数据量就不好控制了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/11/skillness/python文件读写模式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/11/skillness/python文件读写模式/" itemprop="url">python文件读写模式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-11T14:36:26+08:00">
                2019-10-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/skilledness/" itemprop="url" rel="index">
                    <span itemprop="name">skilledness</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>python文件读写模式<br>r : 读取文件，若文件不存在则会报错</p>
<p>w: 写入文件，若文件不存在则会先创建再写入，会覆盖原文件</p>
<p>a : 写入文件，若文件不存在则会先创建再写入，但不会覆盖原文件，而是追加在文件末尾</p>
<p>rb,wb：分别于r,w类似，但是用于读写二进制文件</p>
<p>r+ : 可读、可写，文件不存在也会报错，写操作时会覆盖</p>
<p>w+ : 可读，可写，文件不存在先创建，会覆盖</p>
<p>a+ ：可读、可写，文件不存在先创建，不会覆盖，追加在末尾</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-24/" itemprop="url">2019-7-24</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:49:47+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>经典序列标注任务<br>自然语言任务：分类任务、结果学习任务。<br>词性标注：我（名词） 家 养（动词） 了 只（量词） 猫。标签之间也是有依赖关系的，比如名词后面经常跟动词。</p>
<p>【分词】<br>中文、日语词语词之间是没有间隔的。把分词转换为序列标注问题<br>例子一：“天津大学新校区位于津南区” 进行分词<br>几种分词标签：①Single就是单个标签，比如“猫”②Begin比如“天津大学”③Middle“津大”④End。<br>“小 明 家 养 了 一 只 猫”<br>“B  E  S  B  E  B  E  S”<br>标签之间有一些依赖和限制，比如S不可能接M或者E。<br>通常是在LSTM接一个CRF用来解决标签之间相互转换为关系。比如要对“天津大学新校区位于津南区”进行分词，CRF模型对词进行打分，比如“南”这个字被预测为B E M S的概率。对于“津”的标签预测是和“南”字有关联的，所以在对“津”打分的时候，分为两部分，第一部分是这个字被预测为B E M S的概率，另一部分是前一个词“南”被。<br>求解空间是4的n次方，n是这句话有几个字，每个词被预测为4种标签的概率。假如每个字之间没有任何依赖关系，想得到最好的预测，最优子结果不仅包括每个词的预测，还有每个词之间的联系也就是边的值。<br>怎么做训练：</p>
<p>【补充】<br>Shape属性。查看矩阵或者数组的维数，shape[0] 为第一维的长度，shape[1] 为第二维的长度<br>Reshape()函数重新定义了原张量的阶数：给数组一个新的形状而不改变其数据。通过reshape生成的新数组和原始数组公用一个内存，也就是说，假如更改一个数组的元素，另一个数组也将发生改变 。</p>
<p>a=np.array([1,2,3,4,5,6,7,8,9,10,11,12])<br>b=np.reshape(a,(2,-1))<br>c=np.reshape(a,(2,2,-1))<br>d=np.reshape(a,(2,3,-1))<br>b=<br>[[ 1  2  3  4  5  6]<br> [ 7  8  9  10 11 12]]<br>c=<br>[[[ 1  2  3]<br>  [ 4  5  6]]</p>
<p> [[ 7  8  9]<br>  [10 11 12]]]<br>d=<br>[[[ 1  2]<br>  [ 3  4]<br>  [ 5  6]]</p>
<p> [[ 7  8]<br>  [ 9 10]<br>  [11 12]]]</p>
<p>Squeeze<br>Unsqueeze<br>Size（）</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-23/" itemprop="url">2019-7-23</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:49:17+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>常见文本关系：<br>文本蕴涵（有方向性，侧重在语义，A推出B，B退不出A）、文本相似（文本字面结构上的相似，不是说语义）、文本复述（文本蕴涵的一种特殊情况，A能推出B ，B能推出A）、逻辑推理（侧重在逻辑方面，文本蕴涵侧重在语义方面）</p>
<p>自然语言的一些任务，任务之一文本蕴涵：<br>【一】文本蕴涵（判断两个句子）三种叫法<br>STS semantic sentence similarity 判断两个句子语义相似性<br>RTE 判断两个句子，一个包含另外一个<br>NLI 两个句子，一个前提一个假设，能否通过一个推出另外一个<br>【二】文本蕴涵的三种标签<br>Entailment蕴含关系：意思相同，A推出B，<br>Neural 中性关系：没有任何关系<br>Contradiction反对关系：意义相反，矛盾<br>【三】文本蕴涵应用场景<br>①问答<br>②信息检索<br>③关系识别<br>④知识获取<br>⑤蕴含对生成<br>【四】文本蕴涵任务里面公开的数据集（都是英文的）<br>MSRP 从一些新闻文章中提取的数据集，里面有很多复杂的语义。<br>Quora 本质上是一个知识分享网站，所有的句子都是一个问题，然后判断这两个问题的关系。<br>SICK   在一个网站上发表一些任务，然后各界人来做标注。<br>SNLI   人工书写的英语子对集合。数据集很大。<br>Scitall 有一个问题，多个选项，选择一个正确答案，然后转换为断言陈述。<br>【五】方法<br>TF-IDF 判断两句话的相关性<br>ESIM模型：有两种（双向LSTM、基于树的LSTM）<br>【六】特征<br>词频特征、词嵌入特征</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-22/" itemprop="url">2019-7-22</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:48:57+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>张老师：<br>【一】<br>Embedding就是一个表，包含词和索引。<br>每次查表，找对应词的向量表示，就是一个operation。<br>不管函数多复杂，总是可以通过分解为一些原子操作，比如查表、线性变化、<br>三种基本运算：<br>①Embedding<br>②Wx<br>③w——x——h——RNN（pooling）——hp——wx（分类、线性变化）<br>整个预测，本质上就是一个函数MLP—pooling—BMLP—MLP—embedding。把这个函数打开，就像是一个图，按照图的拓扑顺序。<br>深度学习就是一个复合函数，为每个原子（节点）定义一个input——forward——output——backward——input。<br>整个神经网络就是模仿一个复合函数。<br>机器学习怎么来：<br>目标函数怎么定义：交叉熵。熵越大损失越大。<br>优化损失值：梯度下降。要把损失往回传，<br>定义好MLP的偏导，就是backward，然后沿着拓扑反方向一层一层往前求。复合函数求导就是深度学习的根基。<br>如果自己不写网络都不需要管backward。<br>Forward：Y=wx+b<br>Backward：求导</p>
<p>LSTM、RNN不能并行计算因为每一步都与前面有关联，但是Transform可以并行计算。<br>Op的概念：每个op都有forward、backward。<br>拓扑的概念：每个句子是一棵树，但是多个句子就是图，并行一定要高。<br>优化的概念：不搞机器学习都不用管。</p>
<p>哪些工作取决于编码能力。<br>【二】<br>性能好：①模型本身；②输入有多丰富。<br>词向量获取Pre trainning<br>语言模型任务：通过“我 喜欢 * 苹果”预测“吃”这个词。这样的语料无穷无尽，任何一个合理的句子都会提供一个语料。选取高频的词。最简单的词向量模型CBow，就是一个分类问题，难点就是预测。预训练的语调要够大。<br>Skipgram正好相反，用“吃”这个词预测它的上下文。</p>
<p>Deep bilstm：情感分析，词性标注，实体识别，句法分析，文本蕴涵，……<br>语言模型：几乎无限的预训练语料。</p>
<p>Language Model<br>ELMO目前最好用的词向量。<br>BERT（transform代替lstm）<br>改进：①Lstm有方向，transform没有方向。②③超大规模数据测。④精细的调参技术。<br>吴林志<br>【三】Char Embedding<br>把字符特征加到 表示信息更丰富。拼接字符级特征。<br>字符词向量没有预训练好的表，都是随机初始化的。<br>训练语调统计的字符不包含全部，所以把字符表建立的完备一点。<br>如果embedding_dim是300维度 char_embed_dim是200维度，那最后拼接就是500 维度，包含了字符特征。</p>
<p>标准正太分布，方差<br> 初始化可以用xavier_normal，会根据前一层节点，当前一层有n个节点的时候，根据标准差为根号1/n进行正态分布初始化，当激活函数tanh、s型曲线的时候用这个<br> 初始化也可以用kaiming_normal，对xavier_normal的改进，标准差乘2进行正态分布，当激活函数relu的时候用这个<br>一般用kaiming_normal，因为大多数激活函数是relu</p>
<p>shape是一个属性，size是方法。</p>
<p>Python  train – char_hidden_size 150 –char_embed_dim 50<br>【问题1】<br>每个索引如果是3维度，经过embedding就变成4维<br>【问题2】<br>conv_out =torch.cat( [self._convs(embed) for _convs in self._convs],dim=1).squeeze(dim=2)<br>【问题3】<br>out.reshape(batch_size, max_seq_len ,-1)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-19/" itemprop="url">2019-7-19</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:48:12+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Transformer<br>Seq——seq机器翻译模型<br>中文——encoder编码——decorder解码——英文<br>Attention只是局部提取信息，每个信息之间没有连接起来。循环结构不需要加位置信息，因为序列循环已经包含了位置信息。但是卷积就需要位置信息，序列的先后顺序，因为他学习不到位置信息。可以给每个节点加一个位置信息，positional encoding。</p>
<p>Attention注意力机制：<br>不是一种模型，而是一种思想。有翻译就一定要用到attention。<br>“我喜欢恐怖片”是正面的，因为人的大脑关注的是局部信息，人们注意到的是“喜欢”这个词。深度学习也是从大量信息中选择需要的重要信息。之前的cnn rnn只学习到“喜欢”这个局部特征，没有考虑情感词。怎么体现这个选择性呢，通过权重，当遇到“喜欢”这个词的时候就权重分配大一点。<br>给序列分配权重：<br>通过点乘，或者余弦，去找到下一步的权重。找到权重之后加权求和。</p>
<p>这是一个模型，没有具体含义，在具体问题中才有含义。<br>用一个外部的查询变量，查询序列Q K的相似度（相似度也就是和哪个关联最大），越相似的权重越高，然后施加到V上面。除以根号dk是防止过大。相似度同时除以一个值，不影响相似度。<br>【相似性：从英文（Q）——attention——翻译为法文(K)<br>从历史隐层信息，根据已经翻译的结果，去找序列里面哪个相似度最高的来找下一个，Q里面有学好的上下文历史信息，帮助预测下一步，权重怎么分配是根据已经学好的历史信息】<br>用一个外部的查询变量，先通过点乘找相似度，通过softmax概率化归一化变成概率，使得和为1.然后施加到V上面求和。</p>
<p>Lstm没有学到局部信息，只学习到了序列信息。<br>Self attention：<br>是Q没有用到外部信息，只用到了自身信息。Q=K=V。只寻找序列内部的联系。<br>（I like apple and it is dilicious自己学习，网络把dilicious指向apple。）无视词之间的距离，直接计算距离关系，能够学习到一个句子的内部结构。直接找，不需要从0时刻开始把前面的信息累计起来，比如dilicious直接找apple。卷积就是直接卷积，没有迭代，不需要把前面执行完，比lstm序列模型速度快。<br>点乘可以并行。矩阵乘不能做并行（因为矩阵是行元素乘列元素累加）。能否做并行就看他的局部变量是否是独立的。<br>残差连接：<br>是<br>Encoder编码：<br>是</p>
<p>Maxpooling本身就是一个attention，取最大元素，就是注意力。<br>要用attention需要外部辅助信息，很多时候没有外部信息，更多时候是用自己的信息去产生权重，也就是self attention，attention重点就是权重。方法就是利用末状态的隐藏层信息h——next，h和c都可以，h就是对c做了线性变化，h涵盖了c。</p>
<p>采用attention依旧存在的问题：<br>（1）序列问题存在一个对齐问题，有的序列是做了填充padding的，有可能最后几个元素是填充的，但是也被分了权重，使得原来有元素的位置权重被分了。<br>解决办法：mask<br>在softmax的时候分配权值，控制padding填充部分不分配权值。把padding部分传入负无穷。</p>
<p>使用服务器设置参数–cuda 1</p>
<p>My question：<br>【1】mask，传入负无穷，就可以分配权值为0。-mask就是padding部分，然后填充为负无穷。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-17/" itemprop="url">2019-7-17</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:47:48+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。<br>循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>
<p>捕捉序列信息，循环神经网络，常见的有LSTM，GRU（lstm的变体），RNN。<br>把之前的信息不断累积，通过一个迭代结构，把之前的序列信息，加上当前时刻的信息。历史信息都学到了。加了一个循环的结构。<br>与传统不同的是：<br>引入了一个新的状态累计信息，引入了一个门机制。<br>可能存在的问题：<br>训练时更新权重：梯度爆炸 梯度离散<br>小于0的数越乘越小，或者是越来越大。两种极端。<br>出现这种异常梯度的原因是序列太长的话，就只能学习到几步，梯度障碍。再往前追溯追溯不了。</p>
<p>几种门机制：输入门，输出门，遗忘门（控制前一时刻多少信息流入）</p>
<p>底层的网络能捕获基本信息，层数加深就能捕获到语义信息。双向网络（有的信息是后缀的，必须从两个方向去捕捉语义，比如“衣服 差 的 不行”。BiLSTM）</p>
<p>reverse()是一个list的反转。reversed是一个内置的反转。</p>
<p>Question：<br>【1】什么元祖什么的</p>
<h1 id="lstmcell-grucell-fw-next-不是元祖-h1-c1迭代"><a href="#lstmcell-grucell-fw-next-不是元祖-h1-c1迭代" class="headerlink" title="lstmcell/grucell   fw_next 不是元祖 h1=c1迭代"></a>lstmcell/grucell   fw_next 不是元祖 h1=c1迭代</h1><p>for xi in range(seq_len):<br>    if self._rnn_type == ‘LSTM’:<br>        # lstmcell   fw_next也是元祖h1 c1  h0 c0<br>        h_next,c_next = self._rnn_cell(inputs[xi],fw_next)<br>        # 先拆开后合并，做一个过滤<br>        h_next = h_next * mask[xi]<br>        c_next = c_next * mask[xi]<br>        fw_next = (h_next,c_next)<br>        outputs.append(h_next)<br>    else:<br>      # RNNcell/GRUcell   fw_next 不是元祖 h1=c1迭代<br>      fw_next = self._rnn_cell(inputs[xi], fw_next)<br>      fw_next = fw_next * mask[xi]<br>      outputs.append(fw_next)</p>
<p>【2】掩码填充什么的<br>【3】def _forward(self,inputs,init_hidden,mask):<br>    ‘’’<br>    :param inputs: inputs[seq_len,batch_size,input_size]<br>    :param init_hidden:[batch_size,hidden_size],如果是lstm，则init_hidden类型是元祖<br>    :param mask: inputs[seq_len,batch_size,hidden_size]<br>    :return:<br>    ‘’’<br>也可以这样处理mask  h_next = h_next * mask[xi]+init_hidden[0]<em>(1-mask[xi])<br>【4】#  outputs保留的是中间状态，每个大小都是batch_size</em>hidden_size<br>【5】为什么要返回三维的torch.stack(tuple(outputs),dim=0)<br>【6】bilstm和lstm  rnn的关系，为什么在bilstm里面调用rnn</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-15/" itemprop="url">2019-7-15</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:47:30+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Vocab.py功能：<br>就是对训练集进行word to index和index to word操作。对预训练的文件读取到一个矩阵表里面，同时进行word to index和index to word操作。<br>def get_embedding_weights(self,path):<br>这个方法作用：就是把预训练的语料那个txt文件进行迁移，迁移到一张表，这张表进行了word to index和index to word的索引操作。之后就可以通过这张表的id来引用需要的向量表示。</p>
<p>Word.txt预训练的词向量的文件从哪里来的（预训练好的词向量，word2vec，fasttext（发明者tomos mikolov。Fasttext改进的是用n-gram），glove）<br>为什么要根据外面预训练好的词向量文件建立一个词索引表，不直接在训练语料上建表？（预训练的涵盖的词范围更广，这样测试的时候oov就会少。如果直接用训练语料建表，测试的时候oov会很多。）<br>词向量空间距离反映语义相似度。</p>
<p>我们整体做的是什么：情感分析，文本分类。<br>模型思路：把模型、参数、语料传给分类器，分类器创建一个对象，然后对象有一个train方法，evaluate评估方法。</p>
<p>对待OOV：（1）置0；（2）随机值；（3）求已知向量的平均值</p>
<p>卷积，激活，池化封装为一序列的操作，进行窗口分别为3,4,5大小的三次卷积，然后将结果进行拼接。</p>
<p>把一层层的维度搞清楚，网络就是搞明白了：</p>
<p>卷积是从左到右，从上到下。<br>行是特征，列是索引。要转置一下再卷积。<br>卷积核池化进行维度变化，激活函数不会。<br>维度变化：<br>输入：<br>64<em>100<br>Embedding：<br>64</em>100<em>300<br>转置64</em>300<em>100，<br>然后卷积<br>64</em>75<em>100  64</em>100<em>100   64</em>125<em>100<br>池化：<br>64</em>75<em>1   64</em>100<em>1   64</em>125<em>1<br>拼接：<br>64</em>300<em>1<br>降维输入linear：<br>64</em>300</p>
<p>Batch_size<em>seq_len<br>Batch_size</em>seq_len<em>embedding<br>Batch_size</em>embedding<em>seq_len<br>Batch_size</em>embedding*1(pooling)</p>
<p>①python特点，一切兼为对象<br>②网络只能跑tensor，不能跑其他。 pytorch网络所有参数都是tensor，不存在numpy等其他类型。跑tensor也不单单是跑tensor，实际上跑的是variable变量，variable封装了data、grad梯度、创建方式<br>③在pytorch0.4之后，Tensor和variable合并了。<br>④List要用modulelist，不然不会报错，但是效果会差。<br>⑤模型参数 dilation是指用于控制内核点之间的距离，一般图像卷积会有用，文本卷积不需要。</p>
<p>写模型要看官方文档：<br>Pytorch<br><a href="https://pytorch-cn.readthedocs.io/zh/latest/" target="_blank" rel="noopener">https://pytorch-cn.readthedocs.io/zh/latest/</a><br><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/index.html</a><br>Python<br><a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener">https://www.runoob.com/python3/python3-tutorial.html</a></p>
<p>问题：<br>self._extwd2idx = {wd: idx+1 for idx,wd in enumerate(vec_tabs.keys())}</p>
<p>过拟合就是在训练集上很好，在测试集上很差。本质上就是因为①模型参数太多了②以复杂的模型去拟合少量的数据。<br>抑制过拟合：让数据又多又干净<br>①数据预处理（降噪，）<br>②dropout层（用在训练的时候）<br>③增大数据规模<br>加dropout原因：可能某些节点之间有依赖关系，是两个节点共同作用使结果变好。所以要用dropout随机的断掉一些节点，消除因为某些节点之间的依赖使得模型性能好。 让模型的泛华能力更强。<br>一般模型都加dropout。只有在训练的时候加。<br>④调参技巧：Eearlystopping早停<br>⑤weight_decay权值下降<br>⑥损失函数正则化</p>
<p>调参小结：<br>1.loss看趋势<br>2.训练集acc上升，loss下降表明训练正常<br>3.开发集acc下降，loss上升表明模型过拟合<br>4.Epoch不宜设的过大或过小，过大易过拟合，过小易欠拟合<br>5.Adam学习速率不宜过大（1e-3数量级）<br>6.Batch_size和lr尽量呈正相关<br>7.Dropout设定一般在【0.2,0.5】区间范围内<br>8.调参通过命令行去调整，把需要知道的信息打印出来<br>9.可以尝试使用grid seaarch穷举法调参方式<br>10.K-fold交叉验证</p>
<p>【my question 调优，反向传播，梯度更新，lr，weight decay，随机种子，loss】</p>
<p>Python train.py -h<br>Python Train.py –lr 1e-3 –epoch 32</p>
<p>–batch_size 64 –lr 0.0005  –epoc 20<br>文件里面设置的是默认值，通过命令行调参，看得到修改了哪些值，然后再去修改文件。</p>
<p>Pytorch为什么要梯度初始化置0：<br>梯度下降（权重w怎么更新的，梯度是一个向量，梯度方向是函数上升最快的方向。沿着梯度下降的方向找到局部最小值，对于凸优化问题，局部最小值就是全局最小值。）<br>Pytorch每求一次误差，梯度累加一次。<br>有了u误差更新梯度，有了梯度更新参数</p>
<p>Python关键字yield生成器：每次调用函数会接着上一次执行的结果执行后返回。和return不同，接着取上一次的数据同时还会把后面的函数执行完毕。取生成器里面的数据要通过for来取。</p>
<p>训练数据可能会存在前1000条是一个类别，后1000是一个类别，然后通过batch取得数据的时候取得的都是一个类型。所以在batch取数据的时候要进行shuffle。训练数据越乱越好，尽量包含多个类别数据。</p>
<p>把每个模块的目的搞清楚，然后自己写。</p>
<p>损失函数，用的较多的是MSE，交叉熵<br>weight_decay权重衰减<br>如果预测值w很大，<br>损失函数正则化：求损失的时候再加一个约束项（关于权重的）</p>
<p>My question<br>【一】batch数据变量化，什么二维切片<br>for i,inst in enumerate(batch_data):<br>    seq_len = len(inst.words)<br>    # 把实际长度覆盖掉<br>    wd_idx[i,:seq_len] = torch.tensor(wd_vocab.word2index(inst.words))<br>    lbl_idx[i] = torch.tensor( wd_vocab.label2index(inst.label))</p>
<p>【二】cpu  gpu<br>模型数据要放在同一个设备上。Train和datalodar里面都要加。Args。Device<br>【三】batch_X,batch_y = batch_variable(batch_data,self._vocab,self._args.device)<br>【四】# 用梯度去更新模型参数</p>
<h1 id="①加动量改变梯度方向；②改变学习速率（先快后慢）；"><a href="#①加动量改变梯度方向；②改变学习速率（先快后慢）；" class="headerlink" title="①加动量改变梯度方向；②改变学习速率（先快后慢）；"></a>①加动量改变梯度方向；②改变学习速率（先快后慢）；</h1><p>【五】weight_decay权重衰减<br>【六】# 计算准确率<br>def _calc_acc(self,pred,target):<br>    # pred dim = batch_size,label_size<br>    torch.eq(torch.argmax(pred,dim=1),target)<br>    pass</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/08/暑假笔记/2019-7-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxiaoyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/head.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="丑小鸭的栖息地">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/08/暑假笔记/2019-7-12/" itemprop="url">2019-7-12</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-08T11:46:49+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/暑假培训/" itemprop="url" rel="index">
                    <span itemprop="name">暑假培训</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>【一】黄洪：字符串转换为id，word embedding。<br>预训练的词向量用于embedding。<br>映射：word对应一个id，一个id对应一个词向量。<br>首先预训练出一个词对应向量的词表。<br>然后embedding，pytorch里面。<br>写代码写代码</p>
<p>读取文件，变成string，vocab把word变成数字编号（连续有上界）。<br>Embedding矩阵可以随机初始化；也可以通过外面的词向量预训练得到。<br>【二】杨林：预训练得到词向量（语料库）Embedding怎么来的。<br>fastText提供了集中训练词向量的模型。Fasttext包含了Word2vec。<br>Skipgram模型：找到一个词的上下文信息，也就是语义信息。<br>官方文档参数解释：mincount（规定一个词出现的最小次数，只有达到这个数值的词才可以归到语料库里面）wordNgrams（就是把词比如2gram就是随机把一句话的两个词进行组合。3gram就是随机3个词进行组合。）bucket（和哈希有关系，加快训练查找什么的）minn maxn（规避oov问题）dim（训练的向量维度）lr（learning right在拟合的过程中，首先要在损失函数上面选定一个点，目标是逼近最低点，lr就是长度，每次逼近时移动的长度）loss（损失函数）ws（context window就是怎么定义上下文）epoch（训练多少轮）neg（一般选5）pretrainvector是目标。</p>
<p>【三】吴林志<br>神经网络，首先将输入节点做一个线性或非线性变化，然后激活函数，输出。Y=g(wx+b)。<br>函数就是映射，假设有一个函数f(“cat”)=猫。<br>深度学习最基本的运算就是矩阵。<br>常见激活函数tanh，symoid，relu（公式下去了解）。<br>n多个节点的话：<br>如果每个节点和下一个节点都有连接，就是输入层、隐层、输出层。<br>不全连接，只是部分连接，参数很多，这就是卷积网络（局部连接，权值共享）<br>卷积网络：卷积核（就是输入序列取多少个节点，窗口大小，）<br>卷积操作：加权求和，随着窗口移动对应位置相乘，然后进行卷积结果拼接。</p>
<p>通过窗口局部卷积，卷积一般都是向上取整，填充为了保证输入输出一致。卷积核固定情况下，通过pad做伸缩，二维里面就是保证输出形状基本不变，同时信息提取的更充分。<br>池化层：跟卷积操作一样，不一样的就是对卷积的结果进行处理，一般都是取最大值或者取平均值。<br>经典的卷积：先做一个卷积层，然后做一个线性变化relu（假如卷积信息有效就保留，无用就筛选），最后经过一个池化层（池化层看情况加，大多数情感分析都加池化层。对图像来说，池化目的就是从一堆特征选出一个更好的特征。对于文本来说池化就是得出一个固定的值）。</p>
<p>加激活函数是为了让模型非线性化。不加的话加再多层也只是个线性组合。</p>
<p>总的一个设计思路：<br>1.面向对象、模块化<br>2.设计过程，增量开发<br>3.从整体到局部</p>
<p>Python pytest单元测试：<br>1.py文件以testXXX命名<br>2.方法名也是testXXX</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/head.gif" alt="Chengxiaoyun">
            
              <p class="site-author-name" itemprop="name">Chengxiaoyun</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chengxiaoyun</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
